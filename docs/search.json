[
  {
    "objectID": "index.html#current-work",
    "href": "index.html#current-work",
    "title": "Guy McAtee",
    "section": "",
    "text": "My focus is on the Collections Department of the business, although lately I am assisting with the data migration of a LMS (Loan Management System). Saving money, making money, keeping the money flowing… everyday!\nManage dashboards that use R Shiny and are hosted on the company’s server, Posit Connect\nClean and prepare data from various sources (AWS S3, Presto/Postgres databases)\nAutomate reports (Anomaly detection, daily scripts, PowerPoints)\nCommunicate portfolio health to managers and C-Suite"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Guy McAtee",
    "section": "",
    "text": "BS Information Systems, Business Intelligence 2016-2020"
  },
  {
    "objectID": "index.html#what-can-i-do-for-you",
    "href": "index.html#what-can-i-do-for-you",
    "title": "Guy McAtee",
    "section": "What can I do for you?",
    "text": "What can I do for you?\nA question better suited for you, but I will list some things here:\n\nBuild and maintain ETL processes\n\nAirflow, Presto, PostgreSQL, Posit Connect (RStudio)\n\nBuild interactive and compelling visuals\n\nHighcharts, Plotly, GGPlot etc.\n\nBuild interactive Dashboards in little time\n\nRShiny, Graveler, BS4, ZenDash, Tableau\n\nAnalyze data and tell a story to help stakeholders understand the findings\nExperiment analysis (A/B testing, stats)\nProphet model forecasting"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Guy McAtee",
    "section": "",
    "text": "Build and maintain ETL processes\n\nPresto SQL, PostgreSQL, R, Python, Airflow, Posit Connect (RStudio)\n\nExperiment analysis (A/B testing, stats)\nProphet model forecasting\nBuild interactive and compelling visuals\n\nHighcharts, Plotly, GGPlot etc.\n\nBuild interactive Dashboards\n\nRShiny, Graveler, BS4, ZenDash, Tableau\n\nData storytelling\n\n\n\n\n\n\n\n\n\n Download Resumé\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy focus is on the Collections Department of the business, although lately I am assisting with the data migration of a LMS (Loan Management System). Saving money, making money, keeping the money flowing… everyday!\nManage dashboards that use R Shiny and are hosted on the company’s server, Posit Connect\nClean and prepare data from various sources (AWS S3, Presto/Postgres databases)\nAutomate reports (Anomaly detection, daily scripts, PowerPoints)\nCommunicate portfolio health to managers and C-Suite\n\n\n\n\n\n\n\n\nBS Information Systems, Business Intelligence 2016-2020"
  },
  {
    "objectID": "bring_current.html",
    "href": "bring_current.html",
    "title": "Bring Current Rates",
    "section": "",
    "text": "How long does it take us to recover delinquent Leases? Using a data source that tracks delinquency events (timestamps for delinquncy_ts and recovered_ts) I created a heatmap using ‘reactable’ that shows the rate of recovery in weekly tranches. Most important time is Day 0 (“How many cured the same day they went delinquent?”); in this example about 80% of New Delinquencies are recovered after 9 weeks. Other drilldowns of this data help managers identifty issues in the effort to bring leases back on schedule."
  },
  {
    "objectID": "bring_current.html#step-1-get-the-data",
    "href": "bring_current.html#step-1-get-the-data",
    "title": "Bring Current Rates",
    "section": "Step 1: Get the data",
    "text": "Step 1: Get the data"
  },
  {
    "objectID": "bring_current.html#step-2-write-a-function-to-produce-a-heatmap",
    "href": "bring_current.html#step-2-write-a-function-to-produce-a-heatmap",
    "title": "Bring Current Rates",
    "section": "Step 2: Write a function to produce a heatmap",
    "text": "Step 2: Write a function to produce a heatmap"
  },
  {
    "objectID": "bring_current.html#step-3-create-ui-elements",
    "href": "bring_current.html#step-3-create-ui-elements",
    "title": "Bring Current Rates",
    "section": "Step 3: Create UI elements",
    "text": "Step 3: Create UI elements"
  },
  {
    "objectID": "bring_current.html#steps",
    "href": "bring_current.html#steps",
    "title": "Bring Current Rates",
    "section": "Steps",
    "text": "Steps\n\nSQL Query returning all new delinquencies and a column for recovered_ts\n\n# simplified for the sake of the demo\n bcr_data &lt;- presto(\n   \"\n   SELECT\n     DATE(delinquency_ts AT TIME ZONE 'America/Denver') AS delinquency_dt\n     , DATE(recovered_ts AT TIME ZONE 'America/Denver') AS recovery_dt\n     , de.customer_application_id\n     , CASE WHEN recovered_ts IS NOT NULL THEN\n         DATE_DIFF('day',\n         DATE(delinquency_ts AT TIME ZONE 'America/Denver'),\n         DATE(recovered_ts AT TIME ZONE 'America/Denver'))\n         ELSE NULL\n       END AS days\n   FROM de_table de\n   WHERE DATE(delinquency_ts AT TIME ZONE 'America/Denver') &gt;= date '2020-01-01'\n \")\n\n\n\nSave in Pin (pins data stored on Posit Connect server)\n\npin_write(posit_board, bcr_data, \"bcr_pinned_data\")\n\n\n\nFunction for creating heatmap/line chart (contents of fxn shown)\n\nPals &lt;- list()\n#loop through all levels 'columns' in input df and get mean and sd\nfor(i in levels(df$Age)) {\n  mm &lt;- mean(df$value[df$Age == i])\n  msd &lt;- sd(df$value[df$Age == i])\n  # if column is nd then white bg\n  if(i == 'New Delinquencies') {\n    pal &lt;- list(\n      minColor = 'white',\n      maxColor = 'white'\n    )\n  } else {\n    pal &lt;- list(\n      min = mm - (3 * msd),\n      max = mm + (3 * msd),\n      #color stops\n      stops = list(\n        list(0.15,'#FDE725'),\n        # list(0.16,'#5DC863'),\n        list(0.5,'#21908C'),\n        # list(0.84,'#3B528B'),\n        list(0.85,'#440154')\n      )\n    )\n  }\n  # create list of color palettes\n  Pals &lt;- c(Pals,list(pal))\n}\n  \n  ##### highchart using hcdf data\n  highchart() %&gt;%\n    hc_title(text=paste0('Cumulative Bring Current Rates')) %&gt;%\n    # sdhc_subtitle(text = eval(myHS)) %&gt;% \n    hc_add_series_list(\n      hcdf\n    ) %&gt;%\n    hc_legend(\n      enabled = FALSE\n    ) %&gt;%\n    hc_yAxis(\n      #show max - 16 rows of data\n      min = max(df$y) - 16,\n      max = max(df$y),\n      scrollbar = list(\n        enabled = FALSE\n      ),\n      categories = factor(unique(df$Cohort))\n    ) %&gt;%\n    hc_xAxis(\n      opposite = TRUE,\n      categories = levels(df$Age)\n    ) %&gt;%\n    hc_plotOptions(\n      series = list(\n        dataLabels = list(\n          overflow = 'none',\n          crop = TRUE,\n          enabled = TRUE,\n          #percentages and thousands comma on new delis\n          formatter = JS(\n            \"function() {\n              if (this.point.value == null) {\n                return ''\n              } else if (this.point.x &gt; 0) {\n                return this.point.value + ' %'\n              } else {\n                return Highcharts.numberFormat(this.point.value, 0)\n              };\n            }\"\n          )        \n        )\n      )\n    ) %&gt;% \n    hc_tooltip( # custom tooltip\n        formatter = JS(\n          \"function () {\n            if (this.point.value == null) {\n              return ''\n            } else if (this.point.x &gt; 0) {\n              return 'BCR ' + this.point.value + ' %'\n            } else {\n              return 'New Delinquencies ' + Highcharts.numberFormat(this.point.value, 0)\n            };\n          }\"\n        )\n    )-&gt;p\n  # set color Axis to list of colors in Pals\n  p$x$hc_opts$colorAxis &lt;- Pals\n\n\n\nLine chart using highcharts (contents of fxn shown)\n\n  # hard coding some variables typically used in reactive function\n  cohort_age = 0\n  hardship = \"Exclude\"\n  myRate = 'Cumulative'\n  \n  if(hardship == \"Exclude\") {\n    subtitle = 'Hardship accounts excluded'\n  } else {\n    subtitle = 'Hardship accounts included'\n  }\n  \n  if(myRate == 'Individual'){\n    myRate='BCRate'\n  }else{\n    myRate='BCRateCuml'\n  }\n  \n  # data frame of labels for Year Over Year view\n  my_labels &lt;-  data.frame(\n    date_label = seq(\n      as.Date('2023-01-01'),\n      ceiling_date(Sys.Date(), 'year') - 1,\n      by='week'\n    ) + 1\n  ) %&gt;% \n    mutate(\n      date_label = floor_date(date_label, 'week',week_start = 1),\n      week_num = week(date_label)\n    ) %&gt;% filter(year(date_label)==\"2023\")\n  \n  bca_plot_data &lt;- df %&gt;%\n    na.omit() %&gt;% \n    mutate(\n      year = year(Week),\n      week_num = week(Week),\n      week_actual = Week + weeks(as.integer(DelinquencyAge7) - 1),\n      bcr = ifelse(week_actual &lt;= floor_date(Sys.Date(), 'week', week_start = 1), eval(sym(myRate)), NA)\n    ) %&gt;% \n    filter(\n      year &gt;= year(Sys.Date()) - 3\n    ) %&gt;% \n    merge(\n      my_labels,\n      by = 'week_num'\n    )\n  \n  if (cohort_age == 0) {\n    my_filter &lt;- paste0('Day ',cohort_age)\n    cohort_age_ref &lt;- cohort_age\n  } else if(cohort_age == 10){\n    my_filter &lt;- paste0('Week ',cohort_age,'+')\n    cohort_age_ref &lt;- cohort_age - 1\n  } else {\n    my_filter &lt;- paste0('Week ',cohort_age)\n    cohort_age_ref &lt;- cohort_age - 1\n  }\n  \n  # data prep for highchart\n  hcdf &lt;- bca_plot_data %&gt;% \n    filter(\n      DelinquencyAge7 == eval(my_filter)\n    ) %&gt;% \n    group_by(\n      name = year,\n      type = 'line',\n      color = case_when(\n        year == year(Sys.Date()) ~ \"#FF7214\",\n        year == year(Sys.Date()) -1 ~ \"#003767\",\n        year == year(Sys.Date()) -2 ~ \"#5BC2E7\",\n        year == year(Sys.Date()) -3 ~ \"#CCCCCC\",\n        year == year(Sys.Date()) -4 ~ \"#8DC63F\",\n        year == year(Sys.Date()) -5 ~ \"#8651A1\"\n      ), \n      lineWidth = case_when(\n        year == year(Sys.Date()) ~ 3,\n        year == year(Sys.Date()) - 1 ~ 2,\n        year == year(Sys.Date()) - 2 ~ 2,\n        year ==year(Sys.Date())-3 ~ 2\n      )\n    ) %&gt;% \n    do(data=list_parse(\n      data.frame(\n        x = datetime_to_timestamp(.$date_label),\n        y = round(.$bcr * 100, 2)\n      )\n    ))\n\nhighchart(type = 'stock') %&gt;% \n    hc_title(text = paste0('Day 0 Bring Current Rates')) %&gt;%\n    # sdhc_subtitle(text = subtitle) %&gt;% \n    hc_add_series_list(\n      hcdf\n    ) %&gt;% \n    hc_xAxis(\n      plotLines = list(\n        list(\n          value = datetime_to_timestamp(covid_line), label = list(text='Covid 2020', color = \"#696969\")\n        ),\n        list(\n          value = datetime_to_timestamp(stimulus_line), label = list(text='Stimulus 2020', align = 'left')\n        )\n      )\n    ) %&gt;% \n    hc_tooltip(\n      valueSuffix='%'\n      ) %&gt;% \n    hc_legend(enabled = TRUE) %&gt;% \n    hc_navigator(enabled=FALSE) %&gt;%\n    hc_rangeSelector(enabled=FALSE) %&gt;%\n    hc_scrollbar(enabled=FALSE)   \n\n\n\nThese steps are combined with some Shiny reactive elements in dashboards for switching between options of charts/cohorts etc"
  },
  {
    "objectID": "prophet_model.html#steps",
    "href": "prophet_model.html#steps",
    "title": "Prophet Model Forecast Demo",
    "section": "Steps",
    "text": "Steps"
  },
  {
    "objectID": "prophet_model.html",
    "href": "prophet_model.html",
    "title": "Prophet Model Forecast",
    "section": "",
    "text": "Create a forecast to help a team of phone agents set goals/incentive for each week. The forecast is generated each week and adjusted as necessary. The regressors for the forecast include holidays, day of week, day of month, size of portfolio etc."
  },
  {
    "objectID": "prophet_model.html#script-for-creating-a-prophet-forecast-for-dollars-collected-by-a-team-of-phone-agents",
    "href": "prophet_model.html#script-for-creating-a-prophet-forecast-for-dollars-collected-by-a-team-of-phone-agents",
    "title": "Prophet Model Forecast",
    "section": "Script for creating a Prophet forecast for dollars collected by a team of phone agents",
    "text": "Script for creating a Prophet forecast for dollars collected by a team of phone agents\n\nDefine holidays, first and last day of month\n\n# holidays \nmy_holidays &lt;- presto(\n  \"\n  SELECT * FROM events\n  \"\n) %&gt;% \n  mutate(\n    federal = if_else(holiday == 'New Year', 1, federal),\n    federal = if_else(holiday == \"President's Day\", 1, federal),\n    federal = if_else(holiday == 'Independence Day', 1, federal),\n    federal = if_else(holiday == 'Independence Day Observed', 1, federal),\n    federal = if_else(holiday == \"Veteran's Day\", 1, federal),\n    federal = if_else(holiday == \"Veteran's Day Observed\", 1, federal),\n    federal = if_else(holiday == 'Christmas', 1, federal),\n    federal = if_else(holiday == 'Christmas Observed', 1, federal),\n  )\n\nus_events &lt;- my_holidays %&gt;% \n  select(\n    -row.names,\n    -day_name,\n    -federal\n  )\n\n# paycycles and post holidays\nfom_eom_df &lt;- data.frame(\n  date = seq(as.Date('2017-01-01'), Sys.Date()+365, by='day')\n) %&gt;% \n  rename(\n    DATE = date\n  ) %&gt;% \n  merge(\n    my_holidays %&gt;% \n      rename(DATE = ds) %&gt;% \n      filter(federal == 1) %&gt;% \n      select(\n        DATE,\n        federal\n      ),\n    by = 'DATE',\n    all.x = TRUE\n  ) %&gt;% \n  mutate(\n    dow = weekdays(DATE),\n    day_no = day(as.Date(DATE)),\n    federal = !(is.na(federal)),\n    eom = DATE == ceiling_date(DATE, 'month') - 1,\n    fom = DATE == floor_date(DATE, 'month'),\n    fbdom = case_when(\n      !(dow %in% c('Saturday','Sunday')) & fom == TRUE & federal == FALSE ~ TRUE,\n      lag(dow,1) == 'Sunday' & lag(fom,1) == TRUE & federal == FALSE ~ TRUE,\n      lag(dow,2) == 'Saturday' & lag(fom,2) == TRUE & federal == FALSE ~ TRUE,\n      lag(dow,2) == 'Sunday' & lag(fom,2) == TRUE & lag(federal,1) == TRUE & federal == FALSE ~ TRUE,\n      lag(dow,3) == 'Saturday' & lag(fom,3) == TRUE & lag(federal,1) == TRUE & federal == FALSE ~ TRUE,\n      TRUE ~ FALSE\n    ),\n    lbdom = case_when(\n      !(dow %in% c('Saturday','Sunday')) & eom == TRUE & federal == FALSE ~ TRUE,\n      !(dow %in% c('Saturday','Sunday')) & lead(eom,1) == TRUE & lead(federal,1) == TRUE & federal == FALSE ~ TRUE,\n      lead(dow,1) == 'Saturday' & lead(eom,1) == TRUE & federal == FALSE ~ TRUE,\n      lead(dow,2) == 'Sunday' & lead(eom,2) == TRUE & federal == FALSE ~ TRUE,\n      TRUE ~ FALSE\n    ),\n    mid_mo_payday = case_when(\n      day_no== 15 & !(dow %in% c('Saturday','Sunday')) ~ TRUE,\n      lag(day_no,1) == 15 & lag(dow,1) == 'Sunday' & federal == FALSE ~ TRUE,\n      lag(day_no,2) == 15 & lag(dow,2) == 'Saturday' & federal == FALSE ~ TRUE,\n      lag(day_no,2) == 15 & lag(dow,2) == 'Sunday' & lag(federal,1) == TRUE & federal == FALSE ~ TRUE,\n      lag(day_no,3) == 15 & lag(dow,3) == 'Saturday' & lag(federal,1) == TRUE & federal == FALSE ~ TRUE,\n      TRUE ~ FALSE\n    ),\n    post_holiday = case_when(\n      lag(federal,1) == TRUE & federal == FALSE & !(dow %in% c('Saturday','Sunday')) ~ TRUE,\n      dow == 'Monday' & lag(federal,3) ~ TRUE,\n      TRUE ~ FALSE\n    )\n  )\n\n\n\nGet historical performance\n\n# define number of days to forecast\nforecast_days &lt;- 14\n\nproducts&lt;-c('A','B')\nfor(my_product in products) {\n  print(my_product)\n\n if (my_product == 'A') {\n    my_operator = '='\n    cutoff_dt = as.Date('2017-01-01')\n    dels_dt = cutoff_dt - 14\n  } else {\n    my_operator = '&lt;&gt;'\n    cutoff_dt = as.Date('2019-01-01')\n    dels_dt = cutoff_dt - 14\n  }\n  \n  # get other forecasted data of delinquencies each day to be used as addtl regressor\n  dels_future &lt;- if(my_product == 'A'){\n    pin_get('active_dels_pin', 'rsconnect')$A_cxn %&gt;% rename(ds = day)\n  }else{\n    pin_get('active_dels_pin', 'rsconnect')$B_cxn %&gt;% rename(ds = day)\n  }\n\n    if(my_product == 'A'){\n      cxn_vol &lt;- presto(paste0(\n        \"\n          SELECT ca.payment_dt AS ds\n            , SUM(CAST(payment_amount AS DOUBLE)) AS y\n          FROM payment_data ca\n          WHERE ca.arrangement_status = 'VERIFIED'\n            AND ca.payment_dt &gt;= date '\",cutoff_dt,\"'\n            AND product_type \",my_operator,\" 'A'\n          GROUP BY ca.payment_dt\n        \"\n      )) \n    \n      third_p&lt;-presto(paste0(\n        \"\n          SELECT\n            cat.effective_dt as ds\n            , SUM(CAST(credit_amount AS DOUBLE)) as y\n          FROM transaction_data cat\n          LEFT JOIN accounts ca \n            ON ca.id = cat.account_id\n          WHERE cat.posted_user_id = 12345 --this is third_p id\n            --and 'type' = 'REGULAR_PAYMENT'\n            AND cat.voided_ts is null \n            AND cat.return_reason is null\n            AND cat.effective_dt &gt;= date '2021-06-21'\n            AND cat.effective_dt &lt; current_date\n            AND ca.product_type \",my_operator,\" 'A'\n          GROUP BY cat.effective_dt\n          \"\n    ))\n\n      coll_vol_daily&lt;-cxn_vol %&gt;% rbind(third_p) %&gt;% group_by(ds) %&gt;% summarise(y=sum(y))\n    }else{ # vol for B\n      cxn_vol &lt;- presto(paste0(\n        \"\n          SELECT ca.payment_dt AS ds\n            , SUM(CAST(payment_amount AS DOUBLE)) AS y\n          FROM payments_prod ca\n          WHERE ca.arrangement_status = 'VERIFIED'\n            AND ca.payment_dt &gt;= date '\",cutoff_dt,\"'\n            AND product_type \",my_operator,\" 'A'\n          GROUP BY ca.payment_dt\n        \"\n      ))\n      \n      coll_vol_daily&lt;-cxn_vol %&gt;% group_by(ds) %&gt;% summarise(y=sum(y))\n    }\n\n# prophet for each day -----------------------------------------------------------------\n  days&lt;-0:6\n  DF_all&lt;-data.frame()\n  m_all&lt;-list()\n  forecast_all&lt;-data.frame()\n\n# loop through each day of the week and save forecast to forecast_all\n  for (day in days) {\n  \n    message(paste0(\"starting wday: \", day))\n      \n    mod_coll_data &lt;- coll_vol_daily %&gt;% \n      filter(\n        ds &gt;= as.Date(cutoff_dt)\n      ) %&gt;% \n      mutate(\n        ds = as.Date(ds),\n        dow = weekdays(ds),\n        wday = wday(ds) - 1, # for monday as 0 first day of week\n        month = months(ds),\n        year = year(ds)\n      ) %&gt;% \n      merge(\n        fom_eom_df %&gt;% rename(ds = DATE) %&gt;% select(-dow),\n        by = 'ds',\n        all.x = TRUE\n      ) %&gt;% \n      mutate(\n        saturday = dow == 'Saturday',\n        monday = dow == 'Monday',\n        thursday = dow == 'Thursday',\n        friday = dow == 'Friday',\n        taxseason = week(ds) &gt;= 5 & week(ds) &lt;= 11\n      ) %&gt;% \n      filter(\n        ds &lt; Sys.Date()\n        & !ds %in% c(\n          as.Date('2020-04-15')\n        )\n        & wday == {{ day }}\n      )\n    \n    # future data frame\n    \n    future_append &lt;- data.frame(\n      ds = seq.Date(as.Date(min(mod_coll_data$ds)) ,(as.Date(my_end_date) + forecast_days), by = '1 day') %&gt;%\n        format(\"%Y-%m-%d\") %&gt;%\n        as.Date()\n    ) %&gt;% \n      merge(\n        fom_eom_df %&gt;% \n          mutate(\n            ds = DATE %&gt;% format(\"%Y-%m-%d\") %&gt;% as.Date()\n          ),\n        by = 'ds',\n        all.x = TRUE\n      )  %&gt;% \n      mutate(\n        saturday = dow == 'Saturday',\n        monday = dow == 'Monday',\n        friday = dow == 'Friday',\n        thursday = dow == 'Thursday',\n        taxseason = week(ds) &gt;= 5 & week(ds) &lt;= 11,\n        wday = wday(ds)-1 # for monday as first dow\n      ) %&gt;%\n      merge(\n        dels_future, all = TRUE\n      ) %&gt;% filter(wday == {{ day }})\n    \n    # forecast variables\n    \n    holidayDF = us_events\n    additional_regressors = c(\n      'federal',\n      'post_holiday',\n      'fbdom',\n      'lbdom',\n      'mid_mo_payday',\n      'taxseason',\n      'monday',\n      'friday',\n      'saturday',\n      'delinquencies'#number of active delinquencies\n    ) \n    additional_future_data = list(future_append)\n    interval.width = .8\n    growth_model = 'linear'\n    # growth_model = 'logistic'\n    response_cap = NA\n    response_floor = 0\n    \n    # forecast df ####\n    DF &lt;- mod_coll_data %&gt;% \n      filter(ds &lt;= my_end_date) %&gt;% \n      merge(\n        holidayDF %&gt;% \n          mutate(ds = as.Date(ds %&gt;% format(\"%Y-%m-%d\"))),\n        by = 'ds',\n        all.x = TRUE\n      ) %&gt;% \n      merge(\n        dels_future %&gt;% filter(ds&gt;=as.Date(cutoff_dt)), by = 'ds'\n      ) %&gt;%\n      mutate(\n        cap = response_cap,\n        floor = response_floor\n      ) %&gt;% \n      select(\n        -holiday,\n        -lower_window,\n        -upper_window,\n      ) %&gt;% \n      mutate_if(is.numeric, funs(ifelse(is.na(.), 0, .)))\n    \n    end_date = as.Date(my_end_date)\n    if(is.na(response_cap)) {\n      response_cap = max(DF$y) + 6*(sd(DF$y))\n    }\n    \n    # model setup and additional regressors ####\n    m &lt;- prophet(\n      seasonality.mode = \"multiplicative\"\n      , growth = growth_model # only way to forecast with saturating min or max\n      , holidays = holidayDF %&gt;% filter(as.Date(ds) &lt;= end_date)\n      , interval.width = interval.width\n      , daily.seasonality=FALSE\n      , yearly.seasonality = 13\n      , weekly.seasonality = 3\n    )\n    \n    # add user defined regressors\n    lar &lt;- length(additional_regressors)\n    if(lar &gt; 0) {\n      for(i in 1:lar) {\n        m = add_regressor(m, additional_regressors[i])\n      }\n    }\n\n    m$extra_regressors$saturday$prior.scale &lt;- 10\n    # active dels mode\n    m$extra_regressors$delinquencies$mode &lt;- 'multiplicative'#'additive'\n    \n    # model fit \n    m = fit.prophet(m, DF)\n    \n    # forecast ####\n    # create future data frame on which to run the forecast\n    future &lt;- make_future_dataframe(m, periods = 14) %&gt;% \n      mutate(ds = as.Date(ds %&gt;% format(\"%Y-%m-%d\"))) %&gt;% \n      left_join(\n        holidayDF %&gt;% mutate(ds = as.Date(ds %&gt;% format(\"%Y-%m-%d\"))),\n        by = 'ds'\n      ) %&gt;% \n      filter(\n        wday(ds)-1 == {{ day }}\n      ) %&gt;% \n      mutate(\n        cap = response_cap,\n        floor = response_floor\n      ) %&gt;% \n      select(\n        -holiday,\n        -lower_window,\n        -upper_window\n      ) %&gt;%\n      mutate_if(is.numeric, funs(ifelse(is.na(.), 0, .)))\n    \n    # merge additional future data frames\n    if (length(additional_future_data) &gt; 0) {\n      for(i in 1:length(additional_future_data)) {\n        future &lt;- future %&gt;% \n          merge(\n            additional_future_data[i][[1]],\n            by = 'ds',\n            all.x = TRUE\n          ) %&gt;% \n          mutate_if(is.numeric, funs(ifelse(is.na(.), 0, .)))\n      }\n      \n      future &lt;- future %&gt;% filter(!is.na(federal))\n    }\n    \n    forecast &lt;- predict(m, future, mcmc.samples = 50)\n    # forecast$yhat\n    # dyplot.prophet(m_all,forecast_all)\n    eval(parse_expr(paste0(\"forecast_\",day,\"=forecast\")))\n    message(paste0(\"forecast saved for wday: \", day))     \n    \n    #add days data to all data frame\n    DF_all &lt;- DF_all %&gt;% rbind(DF)\n    m_all &lt;- m_all %&gt;% append(m) #list\n    forecast_all &lt;- forecast_all %&gt;% rbind(forecast)\n  }\n  \n  # output linear model data\n    a1 &lt;- \n      DF_all %&gt;%\n      group_by(day = as.Date(floor_date(ds, \"day\"))) %&gt;%\n      summarise(\n        Actual = sum(y)\n      )\n    \n    b1 &lt;-\n      forecast_all %&gt;%\n      group_by(day = as.Date(floor_date(ds, \"day\"))) %&gt;%\n      summarise(\n        Forecast = round(sum(yhat))\n        ,LowForecast = round(sum(yhat_lower),0)\n        ,HighForecast = round(sum(yhat_upper),0)\n      ) \n  # combine fcst and actuals df  \n    outputDF &lt;- b1 %&gt;% left_join(a1)\n      \n      normHigh_name = paste0('normHigh',as.character(interval.width))\n      normLow_name = paste0('normLow',as.character(interval.width))\n      \n      outputDF &lt;- outputDF %&gt;% \n        mutate(\n          Ratio = outputDF$Actual / outputDF$Forecast,\n          Uncertainty = interval.width,\n          Residual = outputDF$Actual - outputDF$Forecast,\n          !! normHigh_name := outputDF$HighForecast - outputDF$Forecast,\n          !! normLow_name := outputDF$LowForecast - outputDF$Forecast\n        )\n      \n      outputDF$dow = weekdays(outputDF$day)\n      \n  # this week\n    if (offset_bool == FALSE) {\n      my_filter = expr(\n        # day &gt;= ceiling_date(Sys.Date(), 'week', week_start = 1)\n        # & day &lt; ceiling_date(Sys.Date(), 'week', week_start = 1) + 7\n        floor_date(day,'week',week_start =1) == max(floor_date(day,'week',week_start =1)-7)\n      )\n    } else {\n      my_filter = expr(\n        day &gt;= floor_date(Sys.Date(), 'week', week_start = 1) - (week_offset)\n        & day &lt; ceiling_date(Sys.Date(), 'week', week_start = 1) - (week_offset)\n      )\n    }\n  \n  weekly_forecast_data &lt;- outputDF %&gt;%\n    filter(\n      eval(my_filter)\n      # day &lt; '2022-01-24'\n    ) %&gt;%\n    mutate(\n      week = floor_date(day, 'week', week_start = 1),\n      product = my_product\n    ) %&gt;% \n    rename(\n      forecast = Forecast,\n      low_forecast = LowForecast,\n      high_forecast = HighForecast\n    ) %&gt;% \n    select(\n      product,\n      week,\n      day,\n      dow,\n      forecast,\n      low_forecast,\n      high_forecast\n    ) %&gt;% \n    mutate(\n      week = as.character(week)\n      , day = as.character(day)\n      , forecast = as.double(forecast)\n      , low_forecast = as.double(low_forecast)\n      , high_forecast = as.double(high_forecast)\n    )\n    \n  if(test_code == TRUE){\n    \n  }else if(test_code == FALSE){\n    \n    # get current pin\n    pin&lt;-pin_get(\"forecast_cxn_auto\",board=\"rsconnect\")\n    \n    # add new weekly forecast to the pin\n    if(my_product=='A'){\n      pin$AForecast &lt;- pin$AForecast %&gt;% rbind(weekly_forecast_data)\n      \n      # re-pin the data\n    pin(pin, \"forecast_cxn_auto\", board=\"rsconnect\")\n    \n    prior_forecast &lt;- pin_get(\"forecast_cxn_auto\", \"rsconnect\")$AForecast %&gt;%\n      arrange(\n        day\n      ) %&gt;%\n      filter(\n        week == floor_date(max(week)-1, 'week', week_start = 1)\n      )\n    \n    # eval(parse_expr(paste0(my_product,'_prio_forecast_data=prior_forecast')))\n    \n    }else{\n      pin$BForecast &lt;- pin$BForecast %&gt;% rbind(weekly_forecast_data)\n      \n      # re-pin the data\n      pin(pin, \"forecast_cxn_auto\", board=\"rsconnect\")\n      \n      prior_forecast &lt;- pin_get(\"forecast_cxn_auto\", \"rsconnect\")$BForecast %&gt;%\n        arrange(\n          day\n        ) %&gt;%\n        filter(\n          week == floor_date(max(week) - 1, 'week', week_start = 1)\n        )\n    }\n    \n    # print out weekly forecast\n    paste0(my_product,' Forecast: $',sum(weekly_forecast_data$forecast)) %&gt;% print()\n    paste0(my_product,' Low Forecast: $',sum(weekly_forecast_data$low_forecast)) %&gt;% print()\n    paste0(my_product,' High Forecast: $',sum(weekly_forecast_data$high_forecast)) %&gt;% print()\n    \n    # print out prior weekly forecast\n    paste0(my_product,' Prior Forecast: $',sum(prior_forecast$forecast)) %&gt;% print()\n    paste0(my_product,' Prior Low Forecast: $',sum(prior_forecast$low_forecast)) %&gt;% print()\n    paste0(my_product,' Prior High Forecast: $',sum(prior_forecast$high_forecast)) %&gt;% print()\n    \n  }\n  \n}\n\n\n\nStore forecast and actuals in pin\n\nAForecast &lt;- pin_get(\"forecast\",\"rsconnect\")$AForecast\n\ncxn_A_actual &lt;- presto(\n  \"\n  SELECT ca.payment_dt AS day\n    , SUM(CAST(payment_amount AS DOUBLE)) AS actual\n  FROM payments ca\n  WHERE ca.arrangement_status = 'VERIFIED'\n    AND ca.payment_dt &gt;= date '2020-06-21'\n    AND product_type = 'A'\n  GROUP BY ca.payment_dt\n\"\n) \n# week of 6/21/21 we decided to include third_p\nthird_p &lt;- presto(\n  \"\n  SELECT\n    effective_dt as day\n    , SUM(CAST(credit_amount AS DOUBLE)) as actual\n  FROM transactions cat\n  WHERE cat.posted_user_id = 12345 --this is third_p id\n    --and 'type' = 'REGULAR_PAYMENT'\n    and cat.voided_ts is null \n    and cat.return_reason is null\n  -- the week we decided to start including third_p in the actuals\n    and effective_dt &gt;=date '2021-06-21'\n    and effective_dt &lt; current_date\n  GROUP BY effective_dt\n\")\n\nAActual&lt;-cxn_A_actual %&gt;% rbind(third_p) %&gt;% group_by(day) %&gt;% summarise(actual=sum(actual))\n\n\n#get pin\nforecastData &lt;- pins::pin_get(\"forecast\", board = \"rsconnect\")\n\n#update data\nforecastData$AForecast &lt;- AForecast\nforecastData$AActual &lt;- AActual\n\nBActual &lt;- presto(paste0(\n        \"\n          SELECT ca.payment_dt AS day\n            , SUM(CAST(payment_amount AS DOUBLE)) AS actual\n          FROM payments ca\n          WHERE ca.arrangement_status = 'VERIFIED'\n            AND ca.payment_dt &gt;= date '2019-01-01'\n            AND product_type != 'A'\n          GROUP BY ca.payment_dt\n        \"\n      ))\nBForecast &lt;- pin_get(\"forecast\",\"rsconnect\")$BForecast\nforecastData$BForecast &lt;- BForecast\nforecastData$BActual &lt;- BActual\n\n#update pin with updated data\npins::pin(forecastData, name = 'forecast', board = 'rsconnect')"
  },
  {
    "objectID": "prophet_model.html#plot-created-with-highcharts-and-table-with-reactable",
    "href": "prophet_model.html#plot-created-with-highcharts-and-table-with-reactable",
    "title": "Prophet Model Forecast",
    "section": "Plot created with highcharts and table with Reactable",
    "text": "Plot created with highcharts and table with Reactable"
  },
  {
    "objectID": "prophet_model.html#final-product",
    "href": "prophet_model.html#final-product",
    "title": "Prophet Model Forecast",
    "section": "Final Product",
    "text": "Final Product"
  },
  {
    "objectID": "bring_current.html#final-product",
    "href": "bring_current.html#final-product",
    "title": "Bring Current Rates",
    "section": "Final Product",
    "text": "Final Product"
  },
  {
    "objectID": "sql_demo.html",
    "href": "sql_demo.html",
    "title": "SQL",
    "section": "",
    "text": "This query was written to help investigate issues in a software where a certain ip address was disabling the autopay option for many Loans before their first payment date. After validating the query, I automated this query to run every day on the entire portfolio and report back on all times a Loan had their autopay status disabled. This helped dev teams determine the cause for the issue and implement a solution, taking the rate of Loans not making a payment from 20% to 7%.\n\n\n  select \n    sne.entity_id\n    , sne.created\n    , json_extract_scalar(note_data, '$.autopayEnabled.oldValue') as oldValue\n    , json_extract_scalar(note_data, '$.autopayEnabled.newValue') as newValue\n    , sne.note_data\n    , sne.note_title\n    , sne.create_user_name\n    , sne.remote_addr -- ip address of the user that made the change\n  from system_note_entity sne\n  where sne.entity_type = 'Entity.Loan' \n  and sne.reference_type = 'Entity.LoanSettings' -- use this because we know the field is stored there\n  and sne.entity_id = 4988559553\n  and json_extract_scalar(note_data, '$.autopayEnabled.newValue') IS NOT NULL -- filter to where there is a newValue\n\n\n\n\n\nEach hardship has a start and end date. Here you will see how I use a sequence of days along with the hardship data to return the needed results.\n\n\n  with days as (  -- sequence of days since 2019-01-01\n    select\n      cast(day as date) day\n    from\n    (VALUES\n    (SEQUENCE(FROM_ISO8601_DATE('2019-01-01'), \n        FROM_ISO8601_DATE(cast(current_date as varchar)), \n        interval '1' day)\n    )\n    ) AS t1(date_array)\n    cross join\n    unnest(date_array) as t2(day)\n  )\n  , hs_info as ( -- use MAX to get one row per loan\n    select \n        le.id\n        , TRIM(le.display_id) as application_id\n        , MAX(cast(case when cfe.custom_field_id = 160 then cfe.custom_field_value else null end as DATE)) as hardship_start_dt\n      , MAX(cast(case when cfe.custom_field_id = 161 then cfe.custom_field_value else null end as DATE)) as hardship_end_dt\n    from loan_entity le\n    join custom_field__entity cfe \n        on le.settings_id = cfe.entity_id \n      and cfe.entity_type = 'Entity.LoanSettings'\n    join custom_field cf \n        on cf.id = cfe.custom_field_id \n    where le.active = 1 and le.deleted = 0\n    and cfe.deleted = 0 and cf.active = 1\n    and cfe.custom_field_id in (160,161) -- ids for hs start and end dts\n    and cfe.custom_field_value IS not null\n    and cfe.custom_field_value != ''\n    group by le.id, TRIM(le.display_id)\n  )\n  -- join the days and hs_info ctes \n  select \n    dlp.day\n    , COUNT(distinct h.application_id) hs_accts\n  from days d\n  left join hs_info h\n    on d.day between h.hardship_start_dt and h.hardship_end_dt\n  group by dlp.day"
  },
  {
    "objectID": "sql_demo.html#show-each-time-this-loan-had-its-autopay_enabled-field-updated",
    "href": "sql_demo.html#show-each-time-this-loan-had-its-autopay_enabled-field-updated",
    "title": "LoanPro Data Demo",
    "section": "",
    "text": "select \n    sne.entity_id\n    , sne.created\n    , json_extract_scalar(note_data, '$.autopayEnabled.oldValue') as oldValue\n    , json_extract_scalar(note_data, '$.autopayEnabled.newValue') as newValue\n    , sne.note_data\n    , sne.note_title\n    , sne.create_user_name\n    , sne.remote_addr -- ip address of the user that made the change\n  from system_note_entity sne\n  where sne.entity_type = 'Entity.Loan' \n  and sne.reference_type = 'Entity.LoanSettings' -- use this because we know the field is stored there\n  and sne.entity_id = 988595\n  and json_extract_scalar(note_data, '$.autopayEnabled.newValue') IS NOT NULL -- filter to where there is a newValue"
  },
  {
    "objectID": "sql_demo.html#how-many-active-hardship-accounts-every-day",
    "href": "sql_demo.html#how-many-active-hardship-accounts-every-day",
    "title": "LoanPro Data Demo",
    "section": "How many active hardship accounts every day?",
    "text": "How many active hardship accounts every day?"
  },
  {
    "objectID": "prophet_model.html#project",
    "href": "prophet_model.html#project",
    "title": "Prophet Model Forecast",
    "section": "",
    "text": "Create a forecast to help a team of phone agents set goals/incentive for each week. The forecast is generated each week and adjusted as necessary. The regressors for the forecast include holidays, day of week, day of month, size of portfolio etc."
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Guy McAtee",
    "section": "",
    "text": "Build and maintain ETL processes\n\nPresto SQL, PostgreSQL, R, Python, Airflow, Posit Connect (RStudio)\n\nExperiment analysis (A/B testing, stats)\nProphet model forecasting\nBuild interactive and compelling visuals\n\nHighcharts, Plotly, GGPlot etc.\n\nBuild interactive Dashboards\n\nRShiny, Graveler, BS4, ZenDash, Tableau\n\nData storytelling\n\n\n\n\n\n\n\n\n\n Download Resumé"
  },
  {
    "objectID": "bring_current.html#project",
    "href": "bring_current.html#project",
    "title": "Bring Current Rates",
    "section": "",
    "text": "How long does it take us to recover delinquent Leases? Using a data source that tracks delinquency events (timestamps for delinquncy_ts and recovered_ts) I created a heatmap using ‘reactable’ that shows the rate of recovery in weekly tranches. Most important time is Day 0 (“How many cured the same day they went delinquent?”); in this example about 80% of New Delinquencies are recovered after 9 weeks. Other drilldowns of this data help managers identifty issues in the effort to bring leases back on schedule."
  },
  {
    "objectID": "sql_demo.html#loan-management-system-data",
    "href": "sql_demo.html#loan-management-system-data",
    "title": "SQL",
    "section": "",
    "text": "This query was written to help investigate issues in a software where a certain ip address was disabling the autopay option for many Loans before their first payment date. After validating the query, I automated this query to run every day on the entire portfolio and report back on all times a Loan had their autopay status disabled. This helped dev teams determine the cause for the issue and implement a solution, taking the rate of Loans not making a payment from 20% to 7%.\n\n\n  select \n    sne.entity_id\n    , sne.created\n    , json_extract_scalar(note_data, '$.autopayEnabled.oldValue') as oldValue\n    , json_extract_scalar(note_data, '$.autopayEnabled.newValue') as newValue\n    , sne.note_data\n    , sne.note_title\n    , sne.create_user_name\n    , sne.remote_addr -- ip address of the user that made the change\n  from system_note_entity sne\n  where sne.entity_type = 'Entity.Loan' \n  and sne.reference_type = 'Entity.LoanSettings' -- use this because we know the field is stored there\n  and sne.entity_id = 4988559553\n  and json_extract_scalar(note_data, '$.autopayEnabled.newValue') IS NOT NULL -- filter to where there is a newValue\n\n\n\n\n\nEach hardship has a start and end date. Here you will see how I use a sequence of days along with the hardship data to return the needed results.\n\n\n  with days as (  -- sequence of days since 2019-01-01\n    select\n      cast(day as date) day\n    from\n    (VALUES\n    (SEQUENCE(FROM_ISO8601_DATE('2019-01-01'), \n        FROM_ISO8601_DATE(cast(current_date as varchar)), \n        interval '1' day)\n    )\n    ) AS t1(date_array)\n    cross join\n    unnest(date_array) as t2(day)\n  )\n  , hs_info as ( -- use MAX to get one row per loan\n    select \n        le.id\n        , TRIM(le.display_id) as application_id\n        , MAX(cast(case when cfe.custom_field_id = 160 then cfe.custom_field_value else null end as DATE)) as hardship_start_dt\n      , MAX(cast(case when cfe.custom_field_id = 161 then cfe.custom_field_value else null end as DATE)) as hardship_end_dt\n    from loan_entity le\n    join custom_field__entity cfe \n        on le.settings_id = cfe.entity_id \n      and cfe.entity_type = 'Entity.LoanSettings'\n    join custom_field cf \n        on cf.id = cfe.custom_field_id \n    where le.active = 1 and le.deleted = 0\n    and cfe.deleted = 0 and cf.active = 1\n    and cfe.custom_field_id in (160,161) -- ids for hs start and end dts\n    and cfe.custom_field_value IS not null\n    and cfe.custom_field_value != ''\n    group by le.id, TRIM(le.display_id)\n  )\n  -- join the days and hs_info ctes \n  select \n    dlp.day\n    , COUNT(distinct h.application_id) hs_accts\n  from days d\n  left join hs_info h\n    on d.day between h.hardship_start_dt and h.hardship_end_dt\n  group by dlp.day"
  },
  {
    "objectID": "sql_demo.html#determine-if-a-loan-is-past-due-based-on-their-transaction-report",
    "href": "sql_demo.html#determine-if-a-loan-is-past-due-based-on-their-transaction-report",
    "title": "SQL",
    "section": "Determine if a loan is “past due” based on their transaction report",
    "text": "Determine if a loan is “past due” based on their transaction report\n\nUsing the loan transaction report I build a cumulative schedulePayments and compare it to the cumulative payments to determine if someone is “past due” on a given day. There are are several other tables in this query that I used to ensure the loan was past due.\n\n\nwith days as (\n  SELECT\n      CAST(day AS DATE) day\n    FROM\n    (VALUES\n    (SEQUENCE(FROM_ISO8601_DATE('2019-01-01'), \n        FROM_ISO8601_DATE(cast(current_date as VARCHAR)), \n        INTERVAL '1' DAY)\n    )\n    ) AS t1(date_array)\n    CROSS JOIN\n    UNNEST(date_array) AS t2(day)\n  )\n  , cuml_loan as (\n      select d.day\n      , dpd.entity_id as loan_id\n      , TRIM(le.display_id) as application_id\n      , lpv.portfolios_100\n      , dpd.sub_status\n      , dpd.contract_date \n      , lae.apply_date as last_ap_apply_date\n      , lae.process_datetime as last_ap_process_ts\n      , lae.last_ap_amt \n      , lae.freq\n      , coalesce(cast(SUM(pe.amount) over (partition by dpd.entity_id order by d.day) as DOUBLE),0) + coalesce(cast(SUM(c.payment_amount) over (partition by dpd.entity_id order by d.day) as DOUBLE),0) as total_paid\n      -- payments from payment_entity and tx detail\n      , SUM(pe.amount) over (partition by dpd.entity_id order by d.day) as cuml_amt_paid\n      -- credits from loan_tx\n      , SUM(c.payment_amount) over (partition by dpd.entity_id order by d.day) as cuml_credit\n      -- below is the amt owed each day\n      , SUM(s.charge_amount) over (partition by dpd.entity_id order by d.day) as cuml_min_amt\n      -- interest adjustments\n      , SUM(adj.amount) over (partition by dpd.entity_id order by d.day) as cuml_i_adj_amt\n      , coalesce(cast(SUM(s.charge_amount) over (partition by dpd.entity_id order by d.day) as double), 0)+ coalesce(cast(SUM(adj.amount) over (partition by dpd.entity_id order by d.day) AS DOUBLE),0) as cuml_owed_amt\n      from days d\n      join ( -- only return loans that had dpd reset yesterday and their contract date\n          select distinct tx.entity_id, lse1.contract_date, lsse.title as sub_status\n          from  loan_tx tx\n          left join  loan_setup_entity lse1 -- get contract date \n              ON lse1.loan_id = tx.entity_id  \n              AND lse1.deleted = 0 AND lse1.mod_id = 0 \n      left join  loan_settings_entity lse \n        ON lse.loan_id = tx.entity_id and lse.deleted = 0\n      left join  loan_sub_status_entity lsse \n        on lsse.id = lse.loan_sub_status_id and lsse.active = 1 and lsse.deleted = 0\n          where 1=1\n          and tx.title = 'Days Past Due Reset' \n          and tx.date = current_date \n      ) dpd on d.day &gt;= dpd.contract_date\n      left join  loan_entity le on le.id = dpd.entity_id and le.deleted = 0    \n      left join ( -- get all pmts and the amts split by P & I\n          select pe.entity_id, pe.apply_date, SUM(tx.payment_amount) AS amount\n          from  payment_entity pe \n          left join  loan_tx tx on tx.payment_id = pe.id\n          where 1=1 \n              and pe.active = 1 and pe.deleted = 0\n              and reverse_reason is null\n              and reverse_date is null\n        and tx.deleted = 0\n          group by 1,2\n      ) pe on pe.entity_id = dpd.entity_id and d.day=pe.apply_date \n      left join ( -- get the scheduled pmts from loan_tx\n          select tx.entity_id, tx.date, SUM(tx.charge_amount) as charge_amount --, tx.charge_i, tx.charge_p  \n          from  loan_tx tx \n          where tx.type = 'scheduledPayment'\n              and tx.deleted = 0\n          group by 1,2\n      ) s on s.entity_id = dpd.entity_id and s.date = d.day\n      left join ( -- get all credits split by P & I\n          select tx.entity_id,  tx.date, SUM(tx.payment_amount) as payment_amount --, tx.payment_p, tx.payment_i\n          from  loan_tx tx \n          where type = 'credit'\n              and tx.deleted = 0\n      group by 1,2\n      ) c on c.entity_id = dpd.entity_id and c.date = d.day\n      left join ( -- interest adjustments decrease is less money ALICE owes Snap\n          select distinct tx.entity_id,  tx.date, \n          case when json_extract_scalar(info_details, '$.type') = 'decrease' then cast(json_extract_scalar(info_details, '$.amount') as double) * -1 else cast(json_extract_scalar(info_details, '$.amount') as double) end as amount\n          from  loan_tx tx \n          where type = 'intAdjustment'\n              and tx.deleted = 0\n      ) adj on adj.entity_id = dpd.entity_id and adj.date = d.day\n    left join ( -- get latest completed autopay amt for each loan\n        select lae.loan_id, lae.apply_date, lae.process_datetime, lae.recurring_frequency AS freq, dense_rank() over (partition by lae.loan_id order by lae.apply_date desc, lae.amount desc, lae.recurring_frequency) as rn, lae.amount as last_ap_amt\n        from  loan_autopay_entity lae\n        where lae.deleted = 0 and lae.active = 1 \n      and lae.status = 'autopay.status.completed'\n      and lae.apply_date &lt;= current_date\n    ) lae on lae.loan_id = dpd.entity_id and lae.rn =  1\n    left join ( -- get all portfolios in a string\n      select lpv.loan_id, array_join(array_agg(lpv.title), ', ') as portfolios_100\n        from  loan_portfolio_view lpv\n      left join  portfolio_entity lp on lp.id = lpv.id\n        where lpv.active  = 1 \n        group by 1\n    ) lpv on lpv.loan_id = dpd.entity_id \n  )\n  -- get most recent day from cuml and check if past due after summing\n  select \n      cl.day\n      , cl.application_id\n      , cl.loan_id\n    , lse.autopay_enabled\n    , cl.portfolios_100\n      , CASE WHEN COALESCE(cl.cuml_owed_amt, 0) &gt; coalesce(cl.total_paid, 0) then COALESCE(cl.cuml_owed_amt, 0) - coalesce(cl.total_paid, 0) else 0 end as amt_past_due \n      , cl.contract_date\n    , cl.sub_status\n    , cl.cuml_owed_amt\n      , cl.total_paid\n      , CASE WHEN COALESCE(cl.cuml_owed_amt, 0) &gt; coalesce(cl.total_paid, 0) then 'Past Due' else 'Current' end as status\n    , cdt.last_in_dialer_dt\n    , pem.last_succ_pmt_dt\n      , cl.last_ap_amt\n    , cl.last_ap_apply_date\n    , cl.last_ap_process_ts\n    , cl.freq as last_ap_freq\n    , case when lse.autopay_enabled = 0 THEN NULL ELSE np.amount END as next_ap_amt\n    , case when lse.autopay_enabled = 0 THEN NULL ELSE np.process_datetime END as next_ap_process_ts\n    , case when lse.autopay_enabled = 0 THEN NULL ELSE np.apply_date END as next_ap_apply_dt\n    , case when lse.autopay_enabled = 0 THEN NULL ELSE np.freq END as next_ap_freq\n  from cuml_loan cl\n  left join ( -- find out the max dt each loan was in the dialer\n    SELECT application_id, MAX(uploaded_ts) as last_in_dialer_dt\n    from hive.current.snapcollections__contact_data_table cdt \n    WHERE cdt.dialer_status IN ('ready','at_dialer')\n    GROUP BY 1\n  ) cdt on cdt.application_id = cl.application_id \n  left join ( -- next ap details\n    select \n     lae.loan_id, lae.id, lae.amount_type, lae.amount, lae.apply_date , lae.recurring_frequency AS freq, lae.process_datetime, lae.type, lae.status, row_number() over (partition by lae.loan_id order by lae.id ) as rn\n    from  loan_autopay_entity lae\n    where lae.process_datetime &gt;= current_date \n      and lae.status IN ('autopay.status.pending','autopay.status.completed')\n  ) np on np.loan_id = cl.loan_id and np.rn=1\n  left join  loan_settings_entity lse on lse.loan_id = cl.loan_id\n  left join (\n    select max(pe.apply_date) as last_succ_pmt_dt, pe.entity_id\n    from  payment_entity pe\n    where pe.reverse_reason IS NULL AND pe.reverse_date IS NULL and pe.nacha_return_code IS NULL\n    GROUP BY 2\n  ) pem on pem.entity_id = cl.loan_id\n  where cl.day = current_date -- - interval '1' day -- get yesterday for all loans"
  },
  {
    "objectID": "experiment_analysis.html#charge-off-timing-experiment",
    "href": "experiment_analysis.html#charge-off-timing-experiment",
    "title": "Expiriment Analysis (coming soon)",
    "section": "Charge Off Timing Experiment",
    "text": "Charge Off Timing Experiment"
  }
]