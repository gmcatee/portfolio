[
  {
    "objectID": "prophet_model.html",
    "href": "prophet_model.html",
    "title": "Prophet Model Forecast",
    "section": "",
    "text": "Create a forecast to help a team of phone agents set goals/incentive for each week. The forecast is generated each week and adjusted as necessary. The regressors for the forecast include holidays, day of week, day of month, size of portfolio etc."
  },
  {
    "objectID": "prophet_model.html#project",
    "href": "prophet_model.html#project",
    "title": "Prophet Model Forecast",
    "section": "",
    "text": "Create a forecast to help a team of phone agents set goals/incentive for each week. The forecast is generated each week and adjusted as necessary. The regressors for the forecast include holidays, day of week, day of month, size of portfolio etc."
  },
  {
    "objectID": "prophet_model.html#final-product",
    "href": "prophet_model.html#final-product",
    "title": "Prophet Model Forecast",
    "section": "Final Product",
    "text": "Final Product"
  },
  {
    "objectID": "prophet_model.html#script-for-creating-a-prophet-forecast-for-dollars-collected-by-a-team-of-phone-agents",
    "href": "prophet_model.html#script-for-creating-a-prophet-forecast-for-dollars-collected-by-a-team-of-phone-agents",
    "title": "Prophet Model Forecast",
    "section": "Script for creating a Prophet forecast for dollars collected by a team of phone agents",
    "text": "Script for creating a Prophet forecast for dollars collected by a team of phone agents\n\nDefine holidays, first and last day of month\n\n# holidays \nmy_holidays &lt;- presto(\n  \"\n  SELECT * FROM events\n  \"\n) %&gt;% \n  mutate(\n    federal = if_else(holiday == 'New Year', 1, federal),\n    federal = if_else(holiday == \"President's Day\", 1, federal),\n    federal = if_else(holiday == 'Independence Day', 1, federal),\n    federal = if_else(holiday == 'Independence Day Observed', 1, federal),\n    federal = if_else(holiday == \"Veteran's Day\", 1, federal),\n    federal = if_else(holiday == \"Veteran's Day Observed\", 1, federal),\n    federal = if_else(holiday == 'Christmas', 1, federal),\n    federal = if_else(holiday == 'Christmas Observed', 1, federal),\n  )\n\nus_events &lt;- my_holidays %&gt;% \n  select(\n    -row.names,\n    -day_name,\n    -federal\n  )\n\n# paycycles and post holidays\nfom_eom_df &lt;- data.frame(\n  date = seq(as.Date('2017-01-01'), Sys.Date()+365, by='day')\n) %&gt;% \n  rename(\n    DATE = date\n  ) %&gt;% \n  merge(\n    my_holidays %&gt;% \n      rename(DATE = ds) %&gt;% \n      filter(federal == 1) %&gt;% \n      select(\n        DATE,\n        federal\n      ),\n    by = 'DATE',\n    all.x = TRUE\n  ) %&gt;% \n  mutate(\n    dow = weekdays(DATE),\n    day_no = day(as.Date(DATE)),\n    federal = !(is.na(federal)),\n    eom = DATE == ceiling_date(DATE, 'month') - 1,\n    fom = DATE == floor_date(DATE, 'month'),\n    fbdom = case_when(\n      !(dow %in% c('Saturday','Sunday')) & fom == TRUE & federal == FALSE ~ TRUE,\n      lag(dow,1) == 'Sunday' & lag(fom,1) == TRUE & federal == FALSE ~ TRUE,\n      lag(dow,2) == 'Saturday' & lag(fom,2) == TRUE & federal == FALSE ~ TRUE,\n      lag(dow,2) == 'Sunday' & lag(fom,2) == TRUE & lag(federal,1) == TRUE & federal == FALSE ~ TRUE,\n      lag(dow,3) == 'Saturday' & lag(fom,3) == TRUE & lag(federal,1) == TRUE & federal == FALSE ~ TRUE,\n      TRUE ~ FALSE\n    ),\n    lbdom = case_when(\n      !(dow %in% c('Saturday','Sunday')) & eom == TRUE & federal == FALSE ~ TRUE,\n      !(dow %in% c('Saturday','Sunday')) & lead(eom,1) == TRUE & lead(federal,1) == TRUE & federal == FALSE ~ TRUE,\n      lead(dow,1) == 'Saturday' & lead(eom,1) == TRUE & federal == FALSE ~ TRUE,\n      lead(dow,2) == 'Sunday' & lead(eom,2) == TRUE & federal == FALSE ~ TRUE,\n      TRUE ~ FALSE\n    ),\n    mid_mo_payday = case_when(\n      day_no== 15 & !(dow %in% c('Saturday','Sunday')) ~ TRUE,\n      lag(day_no,1) == 15 & lag(dow,1) == 'Sunday' & federal == FALSE ~ TRUE,\n      lag(day_no,2) == 15 & lag(dow,2) == 'Saturday' & federal == FALSE ~ TRUE,\n      lag(day_no,2) == 15 & lag(dow,2) == 'Sunday' & lag(federal,1) == TRUE & federal == FALSE ~ TRUE,\n      lag(day_no,3) == 15 & lag(dow,3) == 'Saturday' & lag(federal,1) == TRUE & federal == FALSE ~ TRUE,\n      TRUE ~ FALSE\n    ),\n    post_holiday = case_when(\n      lag(federal,1) == TRUE & federal == FALSE & !(dow %in% c('Saturday','Sunday')) ~ TRUE,\n      dow == 'Monday' & lag(federal,3) ~ TRUE,\n      TRUE ~ FALSE\n    )\n  )\n\n\n\nGet historical performance\n\n# define number of days to forecast\nforecast_days &lt;- 14\n\nproducts&lt;-c('A','B')\nfor(my_product in products) {\n  print(my_product)\n\n if (my_product == 'A') {\n    my_operator = '='\n    cutoff_dt = as.Date('2017-01-01')\n    dels_dt = cutoff_dt - 14\n  } else {\n    my_operator = '&lt;&gt;'\n    cutoff_dt = as.Date('2019-01-01')\n    dels_dt = cutoff_dt - 14\n  }\n  \n  # get other forecasted data of delinquencies each day to be used as addtl regressor\n  dels_future &lt;- if(my_product == 'A'){\n    pin_get('active_dels_pin', 'rsconnect')$A_cxn %&gt;% rename(ds = day)\n  }else{\n    pin_get('active_dels_pin', 'rsconnect')$B_cxn %&gt;% rename(ds = day)\n  }\n\n    if(my_product == 'A'){\n      cxn_vol &lt;- presto(paste0(\n        \"\n          SELECT ca.payment_dt AS ds\n            , SUM(CAST(payment_amount AS DOUBLE)) AS y\n          FROM payment_data ca\n          WHERE ca.arrangement_status = 'VERIFIED'\n            AND ca.payment_dt &gt;= date '\",cutoff_dt,\"'\n            AND product_type \",my_operator,\" 'A'\n          GROUP BY ca.payment_dt\n        \"\n      )) \n    \n      third_p&lt;-presto(paste0(\n        \"\n          SELECT\n            cat.effective_dt as ds\n            , SUM(CAST(credit_amount AS DOUBLE)) as y\n          FROM transaction_data cat\n          LEFT JOIN accounts ca \n            ON ca.id = cat.account_id\n          WHERE cat.posted_user_id = 12345 --this is third_p id\n            --and 'type' = 'REGULAR_PAYMENT'\n            AND cat.voided_ts is null \n            AND cat.return_reason is null\n            AND cat.effective_dt &gt;= date '2021-06-21'\n            AND cat.effective_dt &lt; current_date\n            AND ca.product_type \",my_operator,\" 'A'\n          GROUP BY cat.effective_dt\n          \"\n    ))\n\n      coll_vol_daily&lt;-cxn_vol %&gt;% rbind(third_p) %&gt;% group_by(ds) %&gt;% summarise(y=sum(y))\n    }else{ # vol for B\n      cxn_vol &lt;- presto(paste0(\n        \"\n          SELECT ca.payment_dt AS ds\n            , SUM(CAST(payment_amount AS DOUBLE)) AS y\n          FROM payments_prod ca\n          WHERE ca.arrangement_status = 'VERIFIED'\n            AND ca.payment_dt &gt;= date '\",cutoff_dt,\"'\n            AND product_type \",my_operator,\" 'A'\n          GROUP BY ca.payment_dt\n        \"\n      ))\n      \n      coll_vol_daily&lt;-cxn_vol %&gt;% group_by(ds) %&gt;% summarise(y=sum(y))\n    }\n\n# prophet for each day -----------------------------------------------------------------\n  days&lt;-0:6\n  DF_all&lt;-data.frame()\n  m_all&lt;-list()\n  forecast_all&lt;-data.frame()\n\n# loop through each day of the week and save forecast to forecast_all\n  for (day in days) {\n  \n    message(paste0(\"starting wday: \", day))\n      \n    mod_coll_data &lt;- coll_vol_daily %&gt;% \n      filter(\n        ds &gt;= as.Date(cutoff_dt)\n      ) %&gt;% \n      mutate(\n        ds = as.Date(ds),\n        dow = weekdays(ds),\n        wday = wday(ds) - 1, # for monday as 0 first day of week\n        month = months(ds),\n        year = year(ds)\n      ) %&gt;% \n      merge(\n        fom_eom_df %&gt;% rename(ds = DATE) %&gt;% select(-dow),\n        by = 'ds',\n        all.x = TRUE\n      ) %&gt;% \n      mutate(\n        saturday = dow == 'Saturday',\n        monday = dow == 'Monday',\n        thursday = dow == 'Thursday',\n        friday = dow == 'Friday',\n        taxseason = week(ds) &gt;= 5 & week(ds) &lt;= 11\n      ) %&gt;% \n      filter(\n        ds &lt; Sys.Date()\n        & !ds %in% c(\n          as.Date('2020-04-15')\n        )\n        & wday == {{ day }}\n      )\n    \n    # future data frame\n    \n    future_append &lt;- data.frame(\n      ds = seq.Date(as.Date(min(mod_coll_data$ds)) ,(as.Date(my_end_date) + forecast_days), by = '1 day') %&gt;%\n        format(\"%Y-%m-%d\") %&gt;%\n        as.Date()\n    ) %&gt;% \n      merge(\n        fom_eom_df %&gt;% \n          mutate(\n            ds = DATE %&gt;% format(\"%Y-%m-%d\") %&gt;% as.Date()\n          ),\n        by = 'ds',\n        all.x = TRUE\n      )  %&gt;% \n      mutate(\n        saturday = dow == 'Saturday',\n        monday = dow == 'Monday',\n        friday = dow == 'Friday',\n        thursday = dow == 'Thursday',\n        taxseason = week(ds) &gt;= 5 & week(ds) &lt;= 11,\n        wday = wday(ds)-1 # for monday as first dow\n      ) %&gt;%\n      merge(\n        dels_future, all = TRUE\n      ) %&gt;% filter(wday == {{ day }})\n    \n    # forecast variables\n    \n    holidayDF = us_events\n    additional_regressors = c(\n      'federal',\n      'post_holiday',\n      'fbdom',\n      'lbdom',\n      'mid_mo_payday',\n      'taxseason',\n      'monday',\n      'friday',\n      'saturday',\n      'delinquencies'#number of active delinquencies\n    ) \n    additional_future_data = list(future_append)\n    interval.width = .8\n    growth_model = 'linear'\n    # growth_model = 'logistic'\n    response_cap = NA\n    response_floor = 0\n    \n    # forecast df ####\n    DF &lt;- mod_coll_data %&gt;% \n      filter(ds &lt;= my_end_date) %&gt;% \n      merge(\n        holidayDF %&gt;% \n          mutate(ds = as.Date(ds %&gt;% format(\"%Y-%m-%d\"))),\n        by = 'ds',\n        all.x = TRUE\n      ) %&gt;% \n      merge(\n        dels_future %&gt;% filter(ds&gt;=as.Date(cutoff_dt)), by = 'ds'\n      ) %&gt;%\n      mutate(\n        cap = response_cap,\n        floor = response_floor\n      ) %&gt;% \n      select(\n        -holiday,\n        -lower_window,\n        -upper_window,\n      ) %&gt;% \n      mutate_if(is.numeric, funs(ifelse(is.na(.), 0, .)))\n    \n    end_date = as.Date(my_end_date)\n    if(is.na(response_cap)) {\n      response_cap = max(DF$y) + 6*(sd(DF$y))\n    }\n    \n    # model setup and additional regressors ####\n    m &lt;- prophet(\n      seasonality.mode = \"multiplicative\"\n      , growth = growth_model # only way to forecast with saturating min or max\n      , holidays = holidayDF %&gt;% filter(as.Date(ds) &lt;= end_date)\n      , interval.width = interval.width\n      , daily.seasonality=FALSE\n      , yearly.seasonality = 13\n      , weekly.seasonality = 3\n    )\n    \n    # add user defined regressors\n    lar &lt;- length(additional_regressors)\n    if(lar &gt; 0) {\n      for(i in 1:lar) {\n        m = add_regressor(m, additional_regressors[i])\n      }\n    }\n\n    m$extra_regressors$saturday$prior.scale &lt;- 10\n    # active dels mode\n    m$extra_regressors$delinquencies$mode &lt;- 'multiplicative'#'additive'\n    \n    # model fit \n    m = fit.prophet(m, DF)\n    \n    # forecast ####\n    # create future data frame on which to run the forecast\n    future &lt;- make_future_dataframe(m, periods = 14) %&gt;% \n      mutate(ds = as.Date(ds %&gt;% format(\"%Y-%m-%d\"))) %&gt;% \n      left_join(\n        holidayDF %&gt;% mutate(ds = as.Date(ds %&gt;% format(\"%Y-%m-%d\"))),\n        by = 'ds'\n      ) %&gt;% \n      filter(\n        wday(ds)-1 == {{ day }}\n      ) %&gt;% \n      mutate(\n        cap = response_cap,\n        floor = response_floor\n      ) %&gt;% \n      select(\n        -holiday,\n        -lower_window,\n        -upper_window\n      ) %&gt;%\n      mutate_if(is.numeric, funs(ifelse(is.na(.), 0, .)))\n    \n    # merge additional future data frames\n    if (length(additional_future_data) &gt; 0) {\n      for(i in 1:length(additional_future_data)) {\n        future &lt;- future %&gt;% \n          merge(\n            additional_future_data[i][[1]],\n            by = 'ds',\n            all.x = TRUE\n          ) %&gt;% \n          mutate_if(is.numeric, funs(ifelse(is.na(.), 0, .)))\n      }\n      \n      future &lt;- future %&gt;% filter(!is.na(federal))\n    }\n    \n    forecast &lt;- predict(m, future, mcmc.samples = 50)\n    # forecast$yhat\n    # dyplot.prophet(m_all,forecast_all)\n    eval(parse_expr(paste0(\"forecast_\",day,\"=forecast\")))\n    message(paste0(\"forecast saved for wday: \", day))     \n    \n    #add days data to all data frame\n    DF_all &lt;- DF_all %&gt;% rbind(DF)\n    m_all &lt;- m_all %&gt;% append(m) #list\n    forecast_all &lt;- forecast_all %&gt;% rbind(forecast)\n  }\n  \n  # output linear model data\n    a1 &lt;- \n      DF_all %&gt;%\n      group_by(day = as.Date(floor_date(ds, \"day\"))) %&gt;%\n      summarise(\n        Actual = sum(y)\n      )\n    \n    b1 &lt;-\n      forecast_all %&gt;%\n      group_by(day = as.Date(floor_date(ds, \"day\"))) %&gt;%\n      summarise(\n        Forecast = round(sum(yhat))\n        ,LowForecast = round(sum(yhat_lower),0)\n        ,HighForecast = round(sum(yhat_upper),0)\n      ) \n  # combine fcst and actuals df  \n    outputDF &lt;- b1 %&gt;% left_join(a1)\n      \n      normHigh_name = paste0('normHigh',as.character(interval.width))\n      normLow_name = paste0('normLow',as.character(interval.width))\n      \n      outputDF &lt;- outputDF %&gt;% \n        mutate(\n          Ratio = outputDF$Actual / outputDF$Forecast,\n          Uncertainty = interval.width,\n          Residual = outputDF$Actual - outputDF$Forecast,\n          !! normHigh_name := outputDF$HighForecast - outputDF$Forecast,\n          !! normLow_name := outputDF$LowForecast - outputDF$Forecast\n        )\n      \n      outputDF$dow = weekdays(outputDF$day)\n      \n  # this week\n    if (offset_bool == FALSE) {\n      my_filter = expr(\n        # day &gt;= ceiling_date(Sys.Date(), 'week', week_start = 1)\n        # & day &lt; ceiling_date(Sys.Date(), 'week', week_start = 1) + 7\n        floor_date(day,'week',week_start =1) == max(floor_date(day,'week',week_start =1)-7)\n      )\n    } else {\n      my_filter = expr(\n        day &gt;= floor_date(Sys.Date(), 'week', week_start = 1) - (week_offset)\n        & day &lt; ceiling_date(Sys.Date(), 'week', week_start = 1) - (week_offset)\n      )\n    }\n  \n  weekly_forecast_data &lt;- outputDF %&gt;%\n    filter(\n      eval(my_filter)\n      # day &lt; '2022-01-24'\n    ) %&gt;%\n    mutate(\n      week = floor_date(day, 'week', week_start = 1),\n      product = my_product\n    ) %&gt;% \n    rename(\n      forecast = Forecast,\n      low_forecast = LowForecast,\n      high_forecast = HighForecast\n    ) %&gt;% \n    select(\n      product,\n      week,\n      day,\n      dow,\n      forecast,\n      low_forecast,\n      high_forecast\n    ) %&gt;% \n    mutate(\n      week = as.character(week)\n      , day = as.character(day)\n      , forecast = as.double(forecast)\n      , low_forecast = as.double(low_forecast)\n      , high_forecast = as.double(high_forecast)\n    )\n    \n  if(test_code == TRUE){\n    \n  }else if(test_code == FALSE){\n    \n    # get current pin\n    pin&lt;-pin_get(\"forecast_cxn_auto\",board=\"rsconnect\")\n    \n    # add new weekly forecast to the pin\n    if(my_product=='A'){\n      pin$AForecast &lt;- pin$AForecast %&gt;% rbind(weekly_forecast_data)\n      \n      # re-pin the data\n    pin(pin, \"forecast_cxn_auto\", board=\"rsconnect\")\n    \n    prior_forecast &lt;- pin_get(\"forecast_cxn_auto\", \"rsconnect\")$AForecast %&gt;%\n      arrange(\n        day\n      ) %&gt;%\n      filter(\n        week == floor_date(max(week)-1, 'week', week_start = 1)\n      )\n    \n    # eval(parse_expr(paste0(my_product,'_prio_forecast_data=prior_forecast')))\n    \n    }else{\n      pin$BForecast &lt;- pin$BForecast %&gt;% rbind(weekly_forecast_data)\n      \n      # re-pin the data\n      pin(pin, \"forecast_cxn_auto\", board=\"rsconnect\")\n      \n      prior_forecast &lt;- pin_get(\"forecast_cxn_auto\", \"rsconnect\")$BForecast %&gt;%\n        arrange(\n          day\n        ) %&gt;%\n        filter(\n          week == floor_date(max(week) - 1, 'week', week_start = 1)\n        )\n    }\n    \n    # print out weekly forecast\n    paste0(my_product,' Forecast: $',sum(weekly_forecast_data$forecast)) %&gt;% print()\n    paste0(my_product,' Low Forecast: $',sum(weekly_forecast_data$low_forecast)) %&gt;% print()\n    paste0(my_product,' High Forecast: $',sum(weekly_forecast_data$high_forecast)) %&gt;% print()\n    \n    # print out prior weekly forecast\n    paste0(my_product,' Prior Forecast: $',sum(prior_forecast$forecast)) %&gt;% print()\n    paste0(my_product,' Prior Low Forecast: $',sum(prior_forecast$low_forecast)) %&gt;% print()\n    paste0(my_product,' Prior High Forecast: $',sum(prior_forecast$high_forecast)) %&gt;% print()\n    \n  }\n  \n}\n\n\n\nStore forecast and actuals in pin\n\nAForecast &lt;- pin_get(\"forecast\",\"rsconnect\")$AForecast\n\ncxn_A_actual &lt;- presto(\n  \"\n  SELECT ca.payment_dt AS day\n    , SUM(CAST(payment_amount AS DOUBLE)) AS actual\n  FROM payments ca\n  WHERE ca.arrangement_status = 'VERIFIED'\n    AND ca.payment_dt &gt;= date '2020-06-21'\n    AND product_type = 'A'\n  GROUP BY ca.payment_dt\n\"\n) \n# week of 6/21/21 we decided to include third_p\nthird_p &lt;- presto(\n  \"\n  SELECT\n    effective_dt as day\n    , SUM(CAST(credit_amount AS DOUBLE)) as actual\n  FROM transactions cat\n  WHERE cat.posted_user_id = 12345 --this is third_p id\n    --and 'type' = 'REGULAR_PAYMENT'\n    and cat.voided_ts is null \n    and cat.return_reason is null\n  -- the week we decided to start including third_p in the actuals\n    and effective_dt &gt;=date '2021-06-21'\n    and effective_dt &lt; current_date\n  GROUP BY effective_dt\n\")\n\nAActual&lt;-cxn_A_actual %&gt;% rbind(third_p) %&gt;% group_by(day) %&gt;% summarise(actual=sum(actual))\n\n\n#get pin\nforecastData &lt;- pins::pin_get(\"forecast\", board = \"rsconnect\")\n\n#update data\nforecastData$AForecast &lt;- AForecast\nforecastData$AActual &lt;- AActual\n\nBActual &lt;- presto(paste0(\n        \"\n          SELECT ca.payment_dt AS day\n            , SUM(CAST(payment_amount AS DOUBLE)) AS actual\n          FROM payments ca\n          WHERE ca.arrangement_status = 'VERIFIED'\n            AND ca.payment_dt &gt;= date '2019-01-01'\n            AND product_type != 'A'\n          GROUP BY ca.payment_dt\n        \"\n      ))\nBForecast &lt;- pin_get(\"forecast\",\"rsconnect\")$BForecast\nforecastData$BForecast &lt;- BForecast\nforecastData$BActual &lt;- BActual\n\n#update pin with updated data\npins::pin(forecastData, name = 'forecast', board = 'rsconnect')"
  },
  {
    "objectID": "prophet_model.html#plot-created-with-highcharts-and-table-with-reactable",
    "href": "prophet_model.html#plot-created-with-highcharts-and-table-with-reactable",
    "title": "Prophet Model Forecast",
    "section": "Plot created with highcharts and table with Reactable",
    "text": "Plot created with highcharts and table with Reactable"
  },
  {
    "objectID": "experiment_analysis.html#charge-off-timing-experiment",
    "href": "experiment_analysis.html#charge-off-timing-experiment",
    "title": "Expiriment Analysis (coming soon)",
    "section": "Charge Off Timing Experiment",
    "text": "Charge Off Timing Experiment"
  },
  {
    "objectID": "sql_demo.html",
    "href": "sql_demo.html",
    "title": "SQL",
    "section": "",
    "text": "This query was written to help investigate issues in a software where a certain ip address was disabling the autopay option for many Loans before their first payment date. After validating the query, I automated this query to run every day on the entire portfolio and report back on all times a Loan had their autopay status disabled. This helped dev teams determine the cause for the issue and implement a solution, taking the rate of Loans not making a payment from 20% to 7%.\n\n\n  select \n    sne.entity_id\n    , sne.created\n    , json_extract_scalar(note_data, '$.autopayEnabled.oldValue') as oldValue\n    , json_extract_scalar(note_data, '$.autopayEnabled.newValue') as newValue\n    , sne.note_data\n    , sne.note_title\n    , sne.create_user_name\n    , sne.remote_addr -- ip address of the user that made the change\n  from system_note_entity sne\n  where sne.entity_type = 'Entity.Loan' \n  and sne.reference_type = 'Entity.LoanSettings' -- use this because we know the field is stored there\n  and sne.entity_id = 4988559553\n  and json_extract_scalar(note_data, '$.autopayEnabled.newValue') IS NOT NULL -- filter to where there is a newValue\n\n\n\n\n\nEach hardship has a start and end date. Here you will see how I use a sequence of days along with the hardship data to return the needed results.\n\n\n  with days as (  -- sequence of days since 2019-01-01\n    select\n      cast(day as date) day\n    from\n    (VALUES\n    (SEQUENCE(FROM_ISO8601_DATE('2019-01-01'), \n        FROM_ISO8601_DATE(cast(current_date as varchar)), \n        interval '1' day)\n    )\n    ) AS t1(date_array)\n    cross join\n    unnest(date_array) as t2(day)\n  )\n  , hs_info as ( -- use MAX to get one row per loan\n    select \n        le.id\n        , TRIM(le.display_id) as application_id\n        , MAX(cast(case when cfe.custom_field_id = 160 then cfe.custom_field_value else null end as DATE)) as hardship_start_dt\n      , MAX(cast(case when cfe.custom_field_id = 161 then cfe.custom_field_value else null end as DATE)) as hardship_end_dt\n    from loan_entity le\n    join custom_field__entity cfe \n        on le.settings_id = cfe.entity_id \n      and cfe.entity_type = 'Entity.LoanSettings'\n    join custom_field cf \n        on cf.id = cfe.custom_field_id \n    where le.active = 1 and le.deleted = 0\n    and cfe.deleted = 0 and cf.active = 1\n    and cfe.custom_field_id in (160,161) -- ids for hs start and end dts\n    and cfe.custom_field_value IS not null\n    and cfe.custom_field_value != ''\n    group by le.id, TRIM(le.display_id)\n  )\n  -- join the days and hs_info ctes \n  select \n    dlp.day\n    , COUNT(distinct h.application_id) hs_accts\n  from days d\n  left join hs_info h\n    on d.day between h.hardship_start_dt and h.hardship_end_dt\n  group by dlp.day"
  },
  {
    "objectID": "sql_demo.html#loan-management-system-data",
    "href": "sql_demo.html#loan-management-system-data",
    "title": "SQL",
    "section": "",
    "text": "This query was written to help investigate issues in a software where a certain ip address was disabling the autopay option for many Loans before their first payment date. After validating the query, I automated this query to run every day on the entire portfolio and report back on all times a Loan had their autopay status disabled. This helped dev teams determine the cause for the issue and implement a solution, taking the rate of Loans not making a payment from 20% to 7%.\n\n\n  select \n    sne.entity_id\n    , sne.created\n    , json_extract_scalar(note_data, '$.autopayEnabled.oldValue') as oldValue\n    , json_extract_scalar(note_data, '$.autopayEnabled.newValue') as newValue\n    , sne.note_data\n    , sne.note_title\n    , sne.create_user_name\n    , sne.remote_addr -- ip address of the user that made the change\n  from system_note_entity sne\n  where sne.entity_type = 'Entity.Loan' \n  and sne.reference_type = 'Entity.LoanSettings' -- use this because we know the field is stored there\n  and sne.entity_id = 4988559553\n  and json_extract_scalar(note_data, '$.autopayEnabled.newValue') IS NOT NULL -- filter to where there is a newValue\n\n\n\n\n\nEach hardship has a start and end date. Here you will see how I use a sequence of days along with the hardship data to return the needed results.\n\n\n  with days as (  -- sequence of days since 2019-01-01\n    select\n      cast(day as date) day\n    from\n    (VALUES\n    (SEQUENCE(FROM_ISO8601_DATE('2019-01-01'), \n        FROM_ISO8601_DATE(cast(current_date as varchar)), \n        interval '1' day)\n    )\n    ) AS t1(date_array)\n    cross join\n    unnest(date_array) as t2(day)\n  )\n  , hs_info as ( -- use MAX to get one row per loan\n    select \n        le.id\n        , TRIM(le.display_id) as application_id\n        , MAX(cast(case when cfe.custom_field_id = 160 then cfe.custom_field_value else null end as DATE)) as hardship_start_dt\n      , MAX(cast(case when cfe.custom_field_id = 161 then cfe.custom_field_value else null end as DATE)) as hardship_end_dt\n    from loan_entity le\n    join custom_field__entity cfe \n        on le.settings_id = cfe.entity_id \n      and cfe.entity_type = 'Entity.LoanSettings'\n    join custom_field cf \n        on cf.id = cfe.custom_field_id \n    where le.active = 1 and le.deleted = 0\n    and cfe.deleted = 0 and cf.active = 1\n    and cfe.custom_field_id in (160,161) -- ids for hs start and end dts\n    and cfe.custom_field_value IS not null\n    and cfe.custom_field_value != ''\n    group by le.id, TRIM(le.display_id)\n  )\n  -- join the days and hs_info ctes \n  select \n    dlp.day\n    , COUNT(distinct h.application_id) hs_accts\n  from days d\n  left join hs_info h\n    on d.day between h.hardship_start_dt and h.hardship_end_dt\n  group by dlp.day"
  },
  {
    "objectID": "sql_demo.html#determine-if-a-loan-is-past-due-based-on-their-transaction-report",
    "href": "sql_demo.html#determine-if-a-loan-is-past-due-based-on-their-transaction-report",
    "title": "SQL",
    "section": "Determine if a loan is “past due” based on their transaction report",
    "text": "Determine if a loan is “past due” based on their transaction report\n\nUsing the loan transaction report I build a cumulative schedulePayments and compare it to the cumulative payments to determine if someone is “past due” on a given day. There are are several other tables in this query that I used to ensure the loan was past due.\n\n\nwith days as (\n  SELECT\n      CAST(day AS DATE) day\n    FROM\n    (VALUES\n    (SEQUENCE(FROM_ISO8601_DATE('2019-01-01'), \n        FROM_ISO8601_DATE(cast(current_date as VARCHAR)), \n        INTERVAL '1' DAY)\n    )\n    ) AS t1(date_array)\n    CROSS JOIN\n    UNNEST(date_array) AS t2(day)\n  )\n  , cuml_loan as (\n      select d.day\n      , dpd.entity_id as loan_id\n      , TRIM(le.display_id) as application_id\n      , lpv.portfolios_100\n      , dpd.sub_status\n      , dpd.contract_date \n      , lae.apply_date as last_ap_apply_date\n      , lae.process_datetime as last_ap_process_ts\n      , lae.last_ap_amt \n      , lae.freq\n      , coalesce(cast(SUM(pe.amount) over (partition by dpd.entity_id order by d.day) as DOUBLE),0) + coalesce(cast(SUM(c.payment_amount) over (partition by dpd.entity_id order by d.day) as DOUBLE),0) as total_paid\n      -- payments from payment_entity and tx detail\n      , SUM(pe.amount) over (partition by dpd.entity_id order by d.day) as cuml_amt_paid\n      -- credits from loan_tx\n      , SUM(c.payment_amount) over (partition by dpd.entity_id order by d.day) as cuml_credit\n      -- below is the amt owed each day\n      , SUM(s.charge_amount) over (partition by dpd.entity_id order by d.day) as cuml_min_amt\n      -- interest adjustments\n      , SUM(adj.amount) over (partition by dpd.entity_id order by d.day) as cuml_i_adj_amt\n      , coalesce(cast(SUM(s.charge_amount) over (partition by dpd.entity_id order by d.day) as double), 0)+ coalesce(cast(SUM(adj.amount) over (partition by dpd.entity_id order by d.day) AS DOUBLE),0) as cuml_owed_amt\n      from days d\n      join ( -- only return loans that had dpd reset yesterday and their contract date\n          select distinct tx.entity_id, lse1.contract_date, lsse.title as sub_status\n          from  loan_tx tx\n          left join  loan_setup_entity lse1 -- get contract date \n              ON lse1.loan_id = tx.entity_id  \n              AND lse1.deleted = 0 AND lse1.mod_id = 0 \n      left join  loan_settings_entity lse \n        ON lse.loan_id = tx.entity_id and lse.deleted = 0\n      left join  loan_sub_status_entity lsse \n        on lsse.id = lse.loan_sub_status_id and lsse.active = 1 and lsse.deleted = 0\n          where 1=1\n          and tx.title = 'Days Past Due Reset' \n          and tx.date = current_date \n      ) dpd on d.day &gt;= dpd.contract_date\n      left join  loan_entity le on le.id = dpd.entity_id and le.deleted = 0    \n      left join ( -- get all pmts and the amts split by P & I\n          select pe.entity_id, pe.apply_date, SUM(tx.payment_amount) AS amount\n          from  payment_entity pe \n          left join  loan_tx tx on tx.payment_id = pe.id\n          where 1=1 \n              and pe.active = 1 and pe.deleted = 0\n              and reverse_reason is null\n              and reverse_date is null\n        and tx.deleted = 0\n          group by 1,2\n      ) pe on pe.entity_id = dpd.entity_id and d.day=pe.apply_date \n      left join ( -- get the scheduled pmts from loan_tx\n          select tx.entity_id, tx.date, SUM(tx.charge_amount) as charge_amount --, tx.charge_i, tx.charge_p  \n          from  loan_tx tx \n          where tx.type = 'scheduledPayment'\n              and tx.deleted = 0\n          group by 1,2\n      ) s on s.entity_id = dpd.entity_id and s.date = d.day\n      left join ( -- get all credits split by P & I\n          select tx.entity_id,  tx.date, SUM(tx.payment_amount) as payment_amount --, tx.payment_p, tx.payment_i\n          from  loan_tx tx \n          where type = 'credit'\n              and tx.deleted = 0\n      group by 1,2\n      ) c on c.entity_id = dpd.entity_id and c.date = d.day\n      left join ( -- interest adjustments decrease is less money ALICE owes Snap\n          select distinct tx.entity_id,  tx.date, \n          case when json_extract_scalar(info_details, '$.type') = 'decrease' then cast(json_extract_scalar(info_details, '$.amount') as double) * -1 else cast(json_extract_scalar(info_details, '$.amount') as double) end as amount\n          from  loan_tx tx \n          where type = 'intAdjustment'\n              and tx.deleted = 0\n      ) adj on adj.entity_id = dpd.entity_id and adj.date = d.day\n    left join ( -- get latest completed autopay amt for each loan\n        select lae.loan_id, lae.apply_date, lae.process_datetime, lae.recurring_frequency AS freq, dense_rank() over (partition by lae.loan_id order by lae.apply_date desc, lae.amount desc, lae.recurring_frequency) as rn, lae.amount as last_ap_amt\n        from  loan_autopay_entity lae\n        where lae.deleted = 0 and lae.active = 1 \n      and lae.status = 'autopay.status.completed'\n      and lae.apply_date &lt;= current_date\n    ) lae on lae.loan_id = dpd.entity_id and lae.rn =  1\n    left join ( -- get all portfolios in a string\n      select lpv.loan_id, array_join(array_agg(lpv.title), ', ') as portfolios_100\n        from  loan_portfolio_view lpv\n      left join  portfolio_entity lp on lp.id = lpv.id\n        where lpv.active  = 1 \n        group by 1\n    ) lpv on lpv.loan_id = dpd.entity_id \n  )\n  -- get most recent day from cuml and check if past due after summing\n  select \n      cl.day\n      , cl.application_id\n      , cl.loan_id\n    , lse.autopay_enabled\n    , cl.portfolios_100\n      , CASE WHEN COALESCE(cl.cuml_owed_amt, 0) &gt; coalesce(cl.total_paid, 0) then COALESCE(cl.cuml_owed_amt, 0) - coalesce(cl.total_paid, 0) else 0 end as amt_past_due \n      , cl.contract_date\n    , cl.sub_status\n    , cl.cuml_owed_amt\n      , cl.total_paid\n      , CASE WHEN COALESCE(cl.cuml_owed_amt, 0) &gt; coalesce(cl.total_paid, 0) then 'Past Due' else 'Current' end as status\n    , cdt.last_in_dialer_dt\n    , pem.last_succ_pmt_dt\n      , cl.last_ap_amt\n    , cl.last_ap_apply_date\n    , cl.last_ap_process_ts\n    , cl.freq as last_ap_freq\n    , case when lse.autopay_enabled = 0 THEN NULL ELSE np.amount END as next_ap_amt\n    , case when lse.autopay_enabled = 0 THEN NULL ELSE np.process_datetime END as next_ap_process_ts\n    , case when lse.autopay_enabled = 0 THEN NULL ELSE np.apply_date END as next_ap_apply_dt\n    , case when lse.autopay_enabled = 0 THEN NULL ELSE np.freq END as next_ap_freq\n  from cuml_loan cl\n  left join ( -- find out the max dt each loan was in the dialer\n    SELECT application_id, MAX(uploaded_ts) as last_in_dialer_dt\n    from hive.current.snapcollections__contact_data_table cdt \n    WHERE cdt.dialer_status IN ('ready','at_dialer')\n    GROUP BY 1\n  ) cdt on cdt.application_id = cl.application_id \n  left join ( -- next ap details\n    select \n     lae.loan_id, lae.id, lae.amount_type, lae.amount, lae.apply_date , lae.recurring_frequency AS freq, lae.process_datetime, lae.type, lae.status, row_number() over (partition by lae.loan_id order by lae.id ) as rn\n    from  loan_autopay_entity lae\n    where lae.process_datetime &gt;= current_date \n      and lae.status IN ('autopay.status.pending','autopay.status.completed')\n  ) np on np.loan_id = cl.loan_id and np.rn=1\n  left join  loan_settings_entity lse on lse.loan_id = cl.loan_id\n  left join (\n    select max(pe.apply_date) as last_succ_pmt_dt, pe.entity_id\n    from  payment_entity pe\n    where pe.reverse_reason IS NULL AND pe.reverse_date IS NULL and pe.nacha_return_code IS NULL\n    GROUP BY 2\n  ) pem on pem.entity_id = cl.loan_id\n  where cl.day = current_date -- - interval '1' day -- get yesterday for all loans"
  },
  {
    "objectID": "bring_current.html",
    "href": "bring_current.html",
    "title": "Bring Current Rates",
    "section": "",
    "text": "How long does it take us to recover delinquent Leases? Using a data source that tracks delinquency events (timestamps for delinquncy_ts and recovered_ts) I created a heatmap using ‘reactable’ that shows the rate of recovery in weekly tranches. Most important time is Day 0 (“How many cured the same day they went delinquent?”); in this example about 80% of New Delinquencies are recovered after 9 weeks. Other drilldowns of this data help managers identifty issues in the effort to bring leases back on schedule."
  },
  {
    "objectID": "bring_current.html#project",
    "href": "bring_current.html#project",
    "title": "Bring Current Rates",
    "section": "",
    "text": "How long does it take us to recover delinquent Leases? Using a data source that tracks delinquency events (timestamps for delinquncy_ts and recovered_ts) I created a heatmap using ‘reactable’ that shows the rate of recovery in weekly tranches. Most important time is Day 0 (“How many cured the same day they went delinquent?”); in this example about 80% of New Delinquencies are recovered after 9 weeks. Other drilldowns of this data help managers identifty issues in the effort to bring leases back on schedule."
  },
  {
    "objectID": "bring_current.html#final-product",
    "href": "bring_current.html#final-product",
    "title": "Bring Current Rates",
    "section": "Final Product",
    "text": "Final Product"
  },
  {
    "objectID": "bring_current.html#steps",
    "href": "bring_current.html#steps",
    "title": "Bring Current Rates",
    "section": "Steps",
    "text": "Steps\n\nSQL Query returning all new delinquencies and a column for recovered_ts\n\n# simplified for the sake of the demo\n bcr_data &lt;- presto(\n   \"\n   SELECT\n     DATE(delinquency_ts AT TIME ZONE 'America/Denver') AS delinquency_dt\n     , DATE(recovered_ts AT TIME ZONE 'America/Denver') AS recovery_dt\n     , de.customer_application_id\n     , CASE WHEN recovered_ts IS NOT NULL THEN\n         DATE_DIFF('day',\n         DATE(delinquency_ts AT TIME ZONE 'America/Denver'),\n         DATE(recovered_ts AT TIME ZONE 'America/Denver'))\n         ELSE NULL\n       END AS days\n   FROM de_table de\n   WHERE DATE(delinquency_ts AT TIME ZONE 'America/Denver') &gt;= date '2020-01-01'\n \")\n\n\n\nSave in Pin (pins data stored on Posit Connect server)\n\npin_write(posit_board, bcr_data, \"bcr_pinned_data\")\n\n\n\nFunction for creating heatmap/line chart (contents of fxn shown)\n\nPals &lt;- list()\n#loop through all levels 'columns' in input df and get mean and sd\nfor(i in levels(df$Age)) {\n  mm &lt;- mean(df$value[df$Age == i])\n  msd &lt;- sd(df$value[df$Age == i])\n  # if column is nd then white bg\n  if(i == 'New Delinquencies') {\n    pal &lt;- list(\n      minColor = 'white',\n      maxColor = 'white'\n    )\n  } else {\n    pal &lt;- list(\n      min = mm - (3 * msd),\n      max = mm + (3 * msd),\n      #color stops\n      stops = list(\n        list(0.15,'#FDE725'),\n        # list(0.16,'#5DC863'),\n        list(0.5,'#21908C'),\n        # list(0.84,'#3B528B'),\n        list(0.85,'#440154')\n      )\n    )\n  }\n  # create list of color palettes\n  Pals &lt;- c(Pals,list(pal))\n}\n  \n  ##### highchart using hcdf data\n  highchart() %&gt;%\n    hc_title(text=paste0('Cumulative Bring Current Rates')) %&gt;%\n    # sdhc_subtitle(text = eval(myHS)) %&gt;% \n    hc_add_series_list(\n      hcdf\n    ) %&gt;%\n    hc_legend(\n      enabled = FALSE\n    ) %&gt;%\n    hc_yAxis(\n      #show max - 16 rows of data\n      min = max(df$y) - 16,\n      max = max(df$y),\n      scrollbar = list(\n        enabled = FALSE\n      ),\n      categories = factor(unique(df$Cohort))\n    ) %&gt;%\n    hc_xAxis(\n      opposite = TRUE,\n      categories = levels(df$Age)\n    ) %&gt;%\n    hc_plotOptions(\n      series = list(\n        dataLabels = list(\n          overflow = 'none',\n          crop = TRUE,\n          enabled = TRUE,\n          #percentages and thousands comma on new delis\n          formatter = JS(\n            \"function() {\n              if (this.point.value == null) {\n                return ''\n              } else if (this.point.x &gt; 0) {\n                return this.point.value + ' %'\n              } else {\n                return Highcharts.numberFormat(this.point.value, 0)\n              };\n            }\"\n          )        \n        )\n      )\n    ) %&gt;% \n    hc_tooltip( # custom tooltip\n        formatter = JS(\n          \"function () {\n            if (this.point.value == null) {\n              return ''\n            } else if (this.point.x &gt; 0) {\n              return 'BCR ' + this.point.value + ' %'\n            } else {\n              return 'New Delinquencies ' + Highcharts.numberFormat(this.point.value, 0)\n            };\n          }\"\n        )\n    )-&gt;p\n  # set color Axis to list of colors in Pals\n  p$x$hc_opts$colorAxis &lt;- Pals\n\n\n\nLine chart using highcharts (contents of fxn shown)\n\n  # hard coding some variables typically used in reactive function\n  cohort_age = 0\n  hardship = \"Exclude\"\n  myRate = 'Cumulative'\n  \n  if(hardship == \"Exclude\") {\n    subtitle = 'Hardship accounts excluded'\n  } else {\n    subtitle = 'Hardship accounts included'\n  }\n  \n  if(myRate == 'Individual'){\n    myRate='BCRate'\n  }else{\n    myRate='BCRateCuml'\n  }\n  \n  # data frame of labels for Year Over Year view\n  my_labels &lt;-  data.frame(\n    date_label = seq(\n      as.Date('2023-01-01'),\n      ceiling_date(Sys.Date(), 'year') - 1,\n      by='week'\n    ) + 1\n  ) %&gt;% \n    mutate(\n      date_label = floor_date(date_label, 'week',week_start = 1),\n      week_num = week(date_label)\n    ) %&gt;% filter(year(date_label)==\"2023\")\n  \n  bca_plot_data &lt;- df %&gt;%\n    na.omit() %&gt;% \n    mutate(\n      year = year(Week),\n      week_num = week(Week),\n      week_actual = Week + weeks(as.integer(DelinquencyAge7) - 1),\n      bcr = ifelse(week_actual &lt;= floor_date(Sys.Date(), 'week', week_start = 1), eval(sym(myRate)), NA)\n    ) %&gt;% \n    filter(\n      year &gt;= year(Sys.Date()) - 3\n    ) %&gt;% \n    merge(\n      my_labels,\n      by = 'week_num'\n    )\n  \n  if (cohort_age == 0) {\n    my_filter &lt;- paste0('Day ',cohort_age)\n    cohort_age_ref &lt;- cohort_age\n  } else if(cohort_age == 10){\n    my_filter &lt;- paste0('Week ',cohort_age,'+')\n    cohort_age_ref &lt;- cohort_age - 1\n  } else {\n    my_filter &lt;- paste0('Week ',cohort_age)\n    cohort_age_ref &lt;- cohort_age - 1\n  }\n  \n  # data prep for highchart\n  hcdf &lt;- bca_plot_data %&gt;% \n    filter(\n      DelinquencyAge7 == eval(my_filter)\n    ) %&gt;% \n    group_by(\n      name = year,\n      type = 'line',\n      color = case_when(\n        year == year(Sys.Date()) ~ \"#FF7214\",\n        year == year(Sys.Date()) -1 ~ \"#003767\",\n        year == year(Sys.Date()) -2 ~ \"#5BC2E7\",\n        year == year(Sys.Date()) -3 ~ \"#CCCCCC\",\n        year == year(Sys.Date()) -4 ~ \"#8DC63F\",\n        year == year(Sys.Date()) -5 ~ \"#8651A1\"\n      ), \n      lineWidth = case_when(\n        year == year(Sys.Date()) ~ 3,\n        year == year(Sys.Date()) - 1 ~ 2,\n        year == year(Sys.Date()) - 2 ~ 2,\n        year ==year(Sys.Date())-3 ~ 2\n      )\n    ) %&gt;% \n    do(data=list_parse(\n      data.frame(\n        x = datetime_to_timestamp(.$date_label),\n        y = round(.$bcr * 100, 2)\n      )\n    ))\n\nhighchart(type = 'stock') %&gt;% \n    hc_title(text = paste0('Day 0 Bring Current Rates')) %&gt;%\n    # sdhc_subtitle(text = subtitle) %&gt;% \n    hc_add_series_list(\n      hcdf\n    ) %&gt;% \n    hc_xAxis(\n      plotLines = list(\n        list(\n          value = datetime_to_timestamp(covid_line), label = list(text='Covid 2020', color = \"#696969\")\n        ),\n        list(\n          value = datetime_to_timestamp(stimulus_line), label = list(text='Stimulus 2020', align = 'left')\n        )\n      )\n    ) %&gt;% \n    hc_tooltip(\n      valueSuffix='%'\n      ) %&gt;% \n    hc_legend(enabled = TRUE) %&gt;% \n    hc_navigator(enabled=FALSE) %&gt;%\n    hc_rangeSelector(enabled=FALSE) %&gt;%\n    hc_scrollbar(enabled=FALSE)   \n\n\n\nThese steps are combined with some Shiny reactive elements in dashboards for switching between options of charts/cohorts etc"
  },
  {
    "objectID": "data_models.html",
    "href": "data_models.html",
    "title": "Data Models",
    "section": "",
    "text": "Stakeholders request: “Recreate the MRR report by customer_id coming from the Revenue Recognition Software (Saas Optics).” The Data Warehouse Team receives data from the Fivetran connector for Saas Optics.\n\n\nValidate the data coming from fivetran\n\ncount of transactions in our RAW tables was &lt; count of transactions in the UI\nCreate separate API connection to confirm we were missing transactions\nDiagnose why we were not receiving all transactions from fivetran\nWork with finance team and DWH team to get all data\nLearn that fivetran only detects deleted transactions on a historical resync\nCreate custom job to historically resync each week to catch transactions\n\nResearch MRR calculation using Saas Optics documentation and Customer Support\n\nRecreate the logic in SQL DBT model to run every morning\nImplement tests for dupes, nulls, etc.\n\nCreate various visualizations using the new table"
  },
  {
    "objectID": "data_models.html#finance-monthly-recurring-revenue",
    "href": "data_models.html#finance-monthly-recurring-revenue",
    "title": "Data Models",
    "section": "",
    "text": "Stakeholders request: “Recreate the MRR report by customer_id coming from the Revenue Recognition Software (Saas Optics).” The Data Warehouse Team receives data from the Fivetran connector for Saas Optics.\n\n\nValidate the data coming from fivetran\n\ncount of transactions in our RAW tables was &lt; count of transactions in the UI\nCreate separate API connection to confirm we were missing transactions\nDiagnose why we were not receiving all transactions from fivetran\nWork with finance team and DWH team to get all data\nLearn that fivetran only detects deleted transactions on a historical resync\nCreate custom job to historically resync each week to catch transactions\n\nResearch MRR calculation using Saas Optics documentation and Customer Support\n\nRecreate the logic in SQL DBT model to run every morning\nImplement tests for dupes, nulls, etc.\n\nCreate various visualizations using the new table"
  },
  {
    "objectID": "data_models.html#sql-query-returning-one-row-for-each-customer_id-each-month",
    "href": "data_models.html#sql-query-returning-one-row-for-each-customer_id-each-month",
    "title": "Data Models",
    "section": "SQL Query returning one row for each customer_id each month",
    "text": "SQL Query returning one row for each customer_id each month\n\nwith transactions_with_mrr as ( -- get all transactions with MRR\n\n  select\n     t.saas_optics_transaction_id\n    , t.order_date\n    , t.transaction_start_date\n    , t.end_date\n    , t.saas_optics_contract_id\n    , case \n        when datediff('month', t.transaction_start_date, t.end_date) = 0 and last_day(t.end_date) != date(t.end_date) then false \n        else true \n      end as mrr_eligible\n      \n    from  base_saas_optics_transaction  t\n    \n    where \n        case  --this logic is used due to the fact mrr is calculated monthly by the last day of the month as the monthly \"cohort\"\n            when datediff('month', t.transaction_start_date, t.end_date) = 0 and last_day(t.end_date) != date(t.end_date) then false \n            else true \n        end = true\n                \n    and t.home_normalized_amount != 0\n    \n)\n, min_txn as ( -- get min txn dates for each customer\n\n    select \n        c.saas_optics_customer_number\n        , min(date(t.transaction_start_date)) as customer_start_date\n    from base_saas_optics_customer c\n\n    left join base_saas_optics_contract ct \n        on c.saas_optics_customer_id = ct.saas_optics_customer_id\n    \n    left join transactions_with_mrr t \n        on t.saas_optics_contract_id = ct.saas_optics_contract_id\n    \n    {{ dbt_utils.group_by(n=1) }}\n\n)\n\n\n, min_txn_parent as ( -- for only parents\n\n      select \n        p.saas_optics_customer_number as parent_number\n        , p.customer_name as parent_name\n        , p.saas_optics_customer_id as parent_id\n        , min(date(t.transaction_start_date)) as parent_start_date\n    \n    from base_saas_optics_customer c\n\n    join base_saas_optics_customer p \n        on p.saas_optics_customer_id = c.parent\n    \n    left join base_saas_optics_contract ct \n        on c.saas_optics_customer_id = ct.saas_optics_customer_id\n\n    left join transactions_with_mrr t \n        on t.saas_optics_contract_id = ct.saas_optics_contract_id \n\n    where c.parent is not null\n\n    {{ dbt_utils.group_by(n=3) }}\n)\n\n, txn_mrr as ( -- \n\n    select  \n        distinct  \n        c.saas_optics_customer_number\n    , c.customer_id\n        , c.customer_name\n        , mt.customer_start_date\n    , coalesce(p.segment, c.segment) as segment -- prefer parent segment\n        , c.parent as customer_parent\n        , p.saas_optics_customer_id as parent_id\n        , coalesce(p.customer_name, '') as parent_name\n        , p.saas_optics_customer_number as parent_number\n        , coalesce(mtp.parent_start_date, psd.parent_start_date) as parent_start_date -- use parents for parent and parents for children\n        , d.last_day_of_month\n        , d2.last_day_of_month as same_month\n    , t.transaction_number as txn_number\n        , t.order_date\n        , t.transaction_start_date\n        , t.end_date\n        , datediff('month', t.transaction_start_date, t.end_date) as month_diff\n        , dateadd(day, -1, date_trunc('month', date(t.end_date)))\n        , t.home_normalized_amount as mrr\n        , mtp.parent_number is not null as parent\n        , c.parent is not null as child\n        , case \n        when c.parent is null and mtp.parent_number is null then true \n        else false \n      end as solo_company\n\n    from base_saas_optics_customer c\n\n    left join base_saas_optics_customer p \n        on p.saas_optics_customer_id = c.parent\n\n    left join min_txn_parent mtp \n        on mtp.parent_number = c.saas_optics_customer_number\n\n  left join min_txn_parent psd -- join again to get childrens parent start dates\n      on psd.parent_id = c.parent\n\n    left join min_txn mt \n        on mt.saas_optics_customer_number = c.saas_optics_customer_number\n\n  left join base_saas_optics_contract ct \n      on c.saas_optics_customer_id = ct.saas_optics_customer_id\n\n  left join base_saas_optics_transaction t\n      on t.saas_optics_contract_id = ct.saas_optics_contract_id\n\n    left join dim_date d \n        on datediff('month', t.transaction_start_date, t.end_date) &gt; 0 -- more than same month\n        and d.last_day_of_month between date_trunc('month', date(t.transaction_start_date)) \n        and date(t.end_date)\n\n    left join dim_date d2\n        on datediff('month', t.transaction_start_date, t.end_date) = 0 -- same month and ends on last day of the month\n        and d2.last_day_of_month = date(t.end_date)\n\n    where mt.customer_start_date is not null\n\n) -- below gets the totals for all children and solo customers\n\n, co_agg as (\n    select \n        tm.parent_name\n        , coalesce(tm.parent_number, '') as parent_number\n        , coalesce(tm.parent_start_date, tm.customer_start_date) as parent_start_date\n        , tm.saas_optics_customer_number\n        , tm.customer_id\n        , tm.customer_name\n        , tm.customer_start_date\n        , coalesce(tm.last_day_of_month, tm.same_month) AS last_day_of_month\n        , tm.parent\n        , tm.child\n        , tm.solo_company\n        , sum(tm.mrr) as mrr\n\n    from txn_mrr tm\n\n    where tm.parent = 'false'\n        \n    {{ dbt_utils.group_by(n=15) }}\n) \n-- get aggregation for all parents\n\n, parent_agg as (\n    select\n        mtp.parent_name\n        , mtp.parent_number\n        , mtp.parent_start_date\n        , mtp.parent_name as customer_name\n        , mtp.parent_number as saas_optics_customer_number\n        , null as customer_id -- no cogs id for parents\n        , mtp.parent_start_date as customer_start_date\n        , ca.last_day_of_month\n        , true as parent\n        , false as child\n        , false as solo_company -- create list of all the childrens segments\n        , sum(mrr) as mrr\n\n    from co_agg ca\n\n    join min_txn_parent mtp \n        on ca.parent_number = mtp.parent_number\n        and ca.last_day_of_month is not null\n\n    {{ dbt_utils.group_by(n=12) }}\n)\n\n-- cte for cleaning up parent segment\n, parent_segment as (\n    select \n        pa.parent_name\n        , pa.parent_number\n        , pa.parent_start_date\n        , pa.customer_name\n        , pa.saas_optics_customer_number\n        , pa.customer_id -- no cogs id for parents\n        , pa.customer_start_date\n        , pa.last_day_of_month\n        , pa.parent\n        , pa.child\n        , pa.solo_company\n        , pa.mrr\n\n    from parent_agg pa \n)\n\nselect \n    ca.customer_name\n    , ca.saas_optics_customer_number\n    , ca.customer_id\n    , ca.parent_start_date\n    , ca.customer_start_date\n    , ca.last_day_of_month\n    , ca.parent as is_parent\n    , ca.child as is_child\n    , ca.solo_company\n    , ca.mrr\n    , datediff('month', date_trunc('month', ca.customer_start_date), ca.last_day_of_month) as month_number\n\nfrom co_agg ca\n\nwhere ca.last_day_of_month &lt;= date_trunc('month', current_date) + interval '1 month' - interval '1 day'\n    and ca.last_day_of_month &gt;= date_trunc('month', customer_start_date)\n\nunion\n\nselect \n    pa.customer_name\n    , pa.saas_optics_customer_number\n    , pa.customer_id\n    , pa.parent_start_date\n    , pa.customer_start_date\n    , pa.last_day_of_month\n    , pa.parent as is_parent\n    , pa.child as is_child\n    , pa.solo_company\n    , pa.mrr\n    , datediff('month', date_trunc('month', pa.customer_start_date), pa.last_day_of_month) as month_number\n\nfrom parent_segment pa \n\nwhere pa.last_day_of_month &lt;= date_trunc('month', current_date) + interval '1 month' - interval '1 day'\n    and pa.last_day_of_month &gt;= date_trunc('month', customer_start_date)\n\n\nPt 2 was to build a Month-Month changes mart using the above fact_finance_mrr table\n\nHow much MRR did we see each month in the New, Upsell, Downsell, Churn categories?\n\n\nwith all_customers as (\n    select \n        distinct customer_name\n        , saas_optics_customer_number\n      , customer_start_date -- used for getting necessary rows later\n      , segment\n\n    from fact_finance_mrr f\n\n    where f.is_child = false -- reports using this data are rolled up to the 'parent' level\n)\n, all_cust_all_months as ( -- r all customers and months bc not all customers exist each month\n    select \n        distinct dd.first_day_of_month\n        , a.saas_optics_customer_number\n        , a.customer_name\n      , a.customer_start_date\n      , a.segment\n      , a.customer_id\n\n    from dim_date  dd\n\n    cross join all_customers a\n\n    where dd.first_day_of_month between '2016-10-01' and date_trunc('month', current_date)\n)\n, agg_mrr as ( -- aggregate to each customer (this is needed for parent customers with multiple segments)\n    select \n        a.saas_optics_customer_number\n    , a.customer_id\n        , a.customer_name\n    , a.customer_start_date\n        , a.first_day_of_month\n        , a.segment\n        , to_number(coalesce(sum(f.mrr),0)) as mrr\n\n    from all_cust_all_months a\n\n    left join fact_finance_mrr f\n        on a.saas_optics_customer_number = f.saas_optics_customer_number\n        and a.first_day_of_month = date_trunc('month', f.last_day_of_month)\n\n    group by 1,2,3,4,5,6\n)\n, min_new_mrr as ( -- get min and max month that eacah customer has MRR\n    select \n        saas_optics_customer_number\n        , min(first_day_of_month) as first_mrr_month\n        , max(first_day_of_month) as last_mrr_month\n\n    from agg_mrr\n\n    where round(mrr,0) != 0 -- would need to change this to give higher max date for those with &lt; $1 MRR\n\n    group by 1\n)\n-- create buckets for saas optics mrr\nselect \n    a.saas_optics_customer_number\n    , a.customer_id\n    , a.customer_name\n    , a.customer_start_date\n    , a.segment\n    , m.first_mrr_month\n    , a.first_day_of_month\n    , coalesce(lag(a.mrr) over (partition by a.saas_optics_customer_number order by a.first_day_of_month), 0) as beginning_mrr_saas_optics\n    , a.mrr as ending_mrr_saas_optics\n    , case \n        when a.first_day_of_month = m.first_mrr_month and a.mrr &gt; 0 and (lag(a.mrr) over (partition by a.saas_optics_customer_number order by a.first_day_of_month) is null\n            or lag(a.mrr) over (partition by a.saas_optics_customer_number order by a.first_day_of_month) = 0)\n        then a.mrr -- (current period)\n        else 0 \n    end as new_mrr_saas_optics\n    , case \n        when a.first_day_of_month &gt; m.first_mrr_month and a.first_day_of_month != m.last_mrr_month + interval '1 month' \n            and coalesce(a.mrr,0) &gt; lag(coalesce(a.mrr,0)) over (partition by a.saas_optics_customer_number order by a.first_day_of_month)\n        then coalesce(a.mrr,0) - lag(coalesce(a.mrr,0)) over (partition by a.saas_optics_customer_number order by a.first_day_of_month) -- (current period - prior)\n        else 0\n    end as upsell_mrr_saas_optics\n    , case \n        when a.first_day_of_month != m.last_mrr_month + interval '1 month' \n            and coalesce(a.mrr,0) &lt; lag(coalesce(a.mrr,0)) over (partition by a.saas_optics_customer_number order by a.first_day_of_month) \n        then (lag(coalesce(a.mrr,0)) over (partition by a.saas_optics_customer_number order by a.first_day_of_month) - coalesce(a.mrr,0)) * -1 -- (prior - current period)\n        else 0\n    end as downsell_mrr_saas_optics\n    , case \n        when a.first_day_of_month = m.last_mrr_month + interval '1 month' \n            and (a.mrr is null or a.mrr = 0 or a.mrr&lt;0)\n        then (lag(a.mrr) over (partition by a.saas_optics_customer_number order by a.first_day_of_month)) * -1 -- (prior period)\n        else 0\n    end as churn_mrr_saas_optics\n\nfrom agg_mrr a\n\nleft join min_new_mrr m \n    on a.saas_optics_customer_number = m.saas_optics_customer_number\n\nwhere a.first_day_of_month &gt;= m.first_mrr_month \nand a.first_day_of_month &lt;= date_trunc('month', current_date)"
  },
  {
    "objectID": "data_models.html#nps-scores",
    "href": "data_models.html#nps-scores",
    "title": "Data Models",
    "section": "NPS Scores",
    "text": "NPS Scores\n\nWhat is our trend in NPS Scores over time? Analysis focused on role of users\n\n\nselect \n    response_uuid\n    , ch.email\n    , co.company_name\n    , 'super user' as role\n    , 'g_sheet_super_user' as source\n    , co.primary_industry \n    , u.score\n    , case \n        when score &lt; 7 then u.improvement_analysis\n        when score between 7 and 8 then concat(u.positive_analysis,' ', u.improvement_analysis)\n        else u.positive_analysis\n      end as score_feedback\n    , date(convert_timezone( 'UTC', 'America/Denver' , u.nps_utc_datetime )) as collected_date\n    , null as user_key --current join is only on email, please update once move to salesforce is completed\n    , co.customer_id\n\nfrom stg_g_sheet_nps_super_user_scores u\n\nleft join stg_intercom_contact_history ch \n    on ch.contact_history_id = trim(regexp_substr(u.intercom_profile, '/users/(.*)', 1, 1, 'e'))\n\nleft join base_hubspot_contact c \n    on c.email  = ch.email\n\nleft join base_hubspot_company co \n    on c.hubspot_company_id = co.hubspot_company_id\n\nwhere ch.email is not null \n\nunion\n\nselect \n    u.response_uuid\n    , ch.email\n    , co.company_name\n    , 'tech' as role\n    , 'g_sheet_tech' as source\n    , co.primary_industry \n    , u.score\n    , case \n        when score &lt; 7 then u.improvement_analysis\n        when score between 7 and 8 then concat(u.positive_analysis,' ', u.improvement_analysis)\n        else u.positive_analysis\n      end as score_feedback\n    , date(convert_timezone( 'UTC', 'America/Denver' , u.nps_utc_datetime )) as collected_date\n    , null as user_key --current join is only on email\n    , co.customer_id\n\nfrom stg_g_sheet_nps_tech_scores u\n\nleft join stg_intercom_contact_history ch \n    on ch.contact_history_id = trim(regexp_substr(u.intercom_profile, '/users/(.*)', 1, 1, 'e'))\n\nleft join base_hubspot_contact c \n    on c.email  = ch.email\n\nleft join base_hubspot_company co \n    on c.hubspot_company_id = co.hubspot_company_id\n\nwhere ch.email is not null \n\nunion\n\nselect\n    response_id\n    , c.user_email\n    , c.company_name\n    , c.user_type\n    , 'beamer' as source\n    , salesforce_account.account_industry \n    , score\n    , feedback\n    , date(c.response_date) as collected_date\n    , md5(concat(c.user_id, ' ', c.customer_id)) as user_key\n    , account.customer_id\n\nfrom stg_nps_responses c\n\nleft join base_salesforce_account account\n    on account.customer_id = c.customer_id\n\nleft join base_salesforce_account salesforce_account \n    on salesforce_account.salesforce_account_id = account.salesforce_account_id\n    \nwhere c.score is not null"
  },
  {
    "objectID": "data_models.html#finance-general-ledger-expenses",
    "href": "data_models.html#finance-general-ledger-expenses",
    "title": "Data Models",
    "section": "Finance General Ledger Expenses",
    "text": "Finance General Ledger Expenses\n\nStakeholders need to know expenses incurred each day\n\n\nSQL Query returning expenses from both ERPs\n\nwith xero_general_ledger_expenses as ( \n\n    select \n        gl.journal_id\n        , gl.source_id\n        , gl.journal_description as transaction_memo\n        , gl.journal_date as transaction_date\n        , gl.account_code as account_id\n        , gl.account_name\n        , gl.contact_name as vendor_name\n        , initcap(lower(gl.account_type)) as account_type_name -- to match title case of netsuite\n        , jl.option as cost_group\n        , jl2.option as department_name \n        , sum(gl.net_amount) as transaction_amount  -- sum by source id first\n\n    from base_xero_general_ledger as gl\n\n    left join base_xero_journal_line_has_tracking_category as jl \n        on gl.journal_line_id = jl.journal_line_id\n        and jl.tracking_category_id = '5d9db1ad-03c9-44bf-997f-a89b48c7d86d'    -- cost groups\n\n    left join base_xero_journal_line_has_tracking_category as jl2 \n        on gl.journal_line_id = jl2.journal_line_id\n        and jl2.tracking_category_id = 'c43eb442-1bcd-4771-80b8-0420de976422'   -- departments\n\n    where gl.journal_date &lt;= '2024-07-31' -- filter for the timerange using XERO as the ERP\n        and gl.account_type = 'EXPENSE' -- show only expenses\n        and gl.account_class = 'EXPENSE'\n\n    group by all\n\n)\n\n, xero_expenses_aggregate as ( \n\n    select \n        transaction_date\n        , transaction_memo\n        , account_id\n        , account_name\n        , account_type_name\n        , department_name\n        , vendor_name\n        , sum(transaction_amount) as transaction_amount\n\n    from xero_general_ledger_expenses\n\n    group by all\n    \n)\n\n, netsuite_expenses_aggregate as (\n\n    select\n        td.transaction_date::date as transaction_date\n            , td.transaction_memo\n        , a.account_search_display_name_copy as account_name\n        , td.account_id \n        , td.account_type_name \n        , regexp_substr(td.department_name , '[^-]+$') as department_name -- account id shown in the name and regex it out to match xero better\n            , td.vendor_name\n        , sum(td.transaction_amount) as transaction_amount\n\n    from base_netsuite_transaction_details as td \n\n    left join base_netsuite_account as a -- to get cleaner name for each account\n        on a.netsuite_account_id = td.account_id \n\n    where td.account_type_name in ('Expense', 'Deferred Expense')\n        and td.transaction_date::date &gt;= '2024-08-01' -- switched to Netsuite as ERP on 8/1/24\n\n    group by all\n\n)\n\nselect \n    {{ dbt_utils.generate_surrogate_key(['transaction_date', 'transaction_memo', 'account_id', 'vendor_name', 'department_name']) }} as general_ledger_expense_key\n    , transaction_date\n    , transaction_memo\n    , account_name\n    , account_id\n    , account_type_name\n    , department_name\n    , vendor_name\n    , transaction_amount\n\nfrom xero_expenses_aggregate\n\nunion\n\nselect\n    {{ dbt_utils.generate_surrogate_key(['transaction_date', 'transaction_memo', 'vendor_name', 'account_id', 'department_name']) }} as general_ledger_expense_key\n    , transaction_date\n    , transaction_memo\n    , account_name\n    , account_id\n    , account_type_name\n    , department_name\n    , vendor_name\n    , transaction_amount\n\nfrom netsuite_expenses_aggregate"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Guy McAtee",
    "section": "",
    "text": "gmact16@gmail.com  Pleasant Grove, UT\nCode for this website: https://github.com/gmcatee/portfolio/tree/main"
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Guy McAtee",
    "section": "Skills",
    "text": "Skills\n\n\nBuild and maintain ETL processes\n\nSnowflake, Presto SQL, PostgreSQL, DBT, R, Python, Airflow, Posit Connect (RStudio)\n\nExperiment analysis (A/B testing, stats)\nProphet model forecasting\nBuild interactive and compelling visuals\n\nHighcharts, Plotly, GGPlot etc.\n\nBuild interactive Dashboards\n\nTableau, RShiny, Graveler, BS4, ZenDash\n\nData storytelling\n\n\n\n\n\n\n\n\n\n Download Resumé"
  },
  {
    "objectID": "index.html#recent-work",
    "href": "index.html#recent-work",
    "title": "Guy McAtee",
    "section": "Recent Work",
    "text": "Recent Work\n\nSenior Business Intelligence Engineer, Limble CMMS\n\nBuilding data solutions for Finance (GTM framework, Revenue Cohort Analysis etc.)\nManage various data models in Snowflake via DBT\nTroubleshoot Five Tran data connections using custom API connectors\n\n\n\nBusiness Intelligence Engineer, Snap! Finance\n\nData migration of a Loan Management System\nManage R Shiny dashboards hosted on the company’s Posit Connect server\nClean and prepare data from various sources (AWS S3, Presto/Postgres databases)\nAutomate reports (Anomaly detection, daily scripts, PowerPoints)\nCommunicate portfolio health to managers and C-Suite"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Guy McAtee",
    "section": "Education",
    "text": "Education\n\nUtah Valley University\n\nBS Information Systems, Business Intelligence 2016-2020"
  },
  {
    "objectID": "python_scripts.html",
    "href": "python_scripts.html",
    "title": "Python Data Scripts",
    "section": "",
    "text": "import os\nimport snowflake.connector\nimport requests\n\n#This script compares customer information to Salesforce, and then writes the correct information into\n#the saas optics API\n\n#The query below compares Chargify data to SFDC, in order to make changes to the incorrect Customer\n#segments, industries and COGS ID's. It then compares back to Saas Optics to check for inconsitencies.\nquery = f'''\nselect c.limble_customer_id as text_field1\n, c.chargify_customer_id \n, a.salesforce_account_id\n, a.segment\n, a.industry\n, cu.limble_customer_id\n, cu.customer_name\n, cu.saas_optics_customer_id\nfrom analytics.base.base_chargify_customer c\nleft join analytics.base.base_salesforce_limble_account la \n    on la.limble_customer_id = c.limble_customer_id \nleft join analytics.base.base_salesforce_account a\n    on a.salesforce_account_id = la.salesforce_account_id \ninner join analytics.base.base_saas_optics_customer cu -- do not return when customer not found in saas optics\n    on cu.saas_optics_customer_number = c.chargify_customer_id \nwhere (cu.is_active or la.customer_status in ('active','past_due','not_paid'))\n-- look for mismatches or where the saas optics field is null\nand ((cu.limble_customer_id != c.limble_customer_id or cu.limble_customer_id is null)\nor (cu.segment != a.segment or cu.segment is null)\nor (cu.industry != a.industry or cu.industry is null))\nand a.salesforce_account_id is not null -- do not update if there is no sfdc account found\nand ((cu.industry is null and a.industry is not null) or (cu.segment is null and a.segment is not null))  -- update when sfdc account is not null'''\n\napi_url = os.environ.get('api_url')\nrev_rec_write_token = os.environ.get('rev_rec_write_token')\n\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Token {rev_rec_write_token}\"\n}\n\n\n\n\n\ndef get_snowflake_data(query):\n    \"\"\"\n    Fetch data from Snowflake based on a given query.\n    \n    Parameters:\n        query (str): The SQL query to execute.\n        \n    Returns:\n        list: List of dictionaries containing query results.\n    \"\"\"\n    # Establish connection to Snowflake\n    conn = snowflake.connector.connect(\n        user = os.environ.get('snowflake_user'),\n        password = os.environ.get('snowflake_password'),\n        account = 'bingbong',\n        warehouse = 'transforming',\n        database = 'l_db',\n        schema = 'base'\n    )\n    \n    # Execute query\n    cursor = conn.cursor()\n    # Query for pulling the data from sfdc snowflake\n    cursor.execute(query)\n    \n    # Fetch results\n    results = cursor.fetchall()\n    columns = [col[0] for col in cursor.description]\n    \n    # Close connection\n    cursor.close()\n    conn.close()\n    \n    # Convert results to list of dictionaries\n    data = [dict(zip(columns, row)) for row in results]\n    return data\n\n\n\n\n\ndef partial_update_customer(api_url, saas_optics_customer_id, headers, name, **kwargs):\n    \"\"\"\n    Updates a specific customer resource using a PATCH request.\n    Parameters:\n        api_url (str): The base URL of the API.\n        resource_id (str): The ID of the customer resource to update.\n        headers (dict): The headers to include in the request (e.g., for authentication).\n        name (str): The name of the customer (required).\n        kwargs: Other fields to update (key-value pairs).\n    Returns:\n        dict: The response from the API if successful.\n        None: If the update fails.\n    \"\"\"\n    # Ensure the 'name' field is included in the data\n    data = {'name': name}\n\n    # Add any other fields to update\n    data.update(kwargs)\n\n    # Make the PATCH request\n    response = requests.patch(f\"{api_url}/{saas_optics_customer_id}/\", headers=headers, json=data)\n    # Log the response for debugging\n    print(f\"Status Code for ID {saas_optics_customer_id}: {response.status_code}\")\n    print(f\"Response Content for ID {saas_optics_customer_id}: {response.content.decode('utf-8')}\")\n    # Check the response\n    if response.status_code == 200:\n        return response.json()\n    else:\n        print(\"Failed to update:\", response.status_code, response.text)\n        return None\n\n\n\n\n\ndef update_customers_from_snowflake(api_url, headers, query):\n    \"\"\"\n    Fetch customer data from Snowflake and update each customer using the API.\n    Parameters:\n        api_url (str): The base URL of the API.\n        headers (dict): The headers to include in the request (e.g., for authentication).\n        query (str): The SQL query to fetch customer data from Snowflake.\n    \"\"\"\n    # Fetch data from Snowflake\n    customer_data = get_snowflake_data(query)\n    print(customer_data)\n    # Iterate through each customer record and update via API\n    for customer in customer_data:\n        print(customer)\n        saas_optics_customer_id = customer['SAAS_OPTICS_CUSTOMER_ID']\n        name = customer['CUSTOMER_NAME']\n        additional_fields = {key.lower(): value for key, value in customer.items() if key not in ['SAAS_OPTICS_CUSTOMER_ID', 'CUSTOMER_NAME']}\n        print(additional_fields)\n        response = partial_update_customer(api_url, saas_optics_customer_id, headers, name, **additional_fields)\n        if response:\n            print(\"Update successful for ID:\", saas_optics_customer_id)\n        else:\n            print(\"Update failed for ID:\", saas_optics_customer_id)\n\n# Run the method and see the results printed out\n# update_customers_from_snowflake(api_url, headers, query)"
  },
  {
    "objectID": "python_scripts.html#sync-revenue-recognition-data-with-salesforce-sfdc",
    "href": "python_scripts.html#sync-revenue-recognition-data-with-salesforce-sfdc",
    "title": "Python Data Scripts",
    "section": "",
    "text": "import os\nimport snowflake.connector\nimport requests\n\n#This script compares customer information to Salesforce, and then writes the correct information into\n#the saas optics API\n\n#The query below compares Chargify data to SFDC, in order to make changes to the incorrect Customer\n#segments, industries and COGS ID's. It then compares back to Saas Optics to check for inconsitencies.\nquery = f'''\nselect c.limble_customer_id as text_field1\n, c.chargify_customer_id \n, a.salesforce_account_id\n, a.segment\n, a.industry\n, cu.limble_customer_id\n, cu.customer_name\n, cu.saas_optics_customer_id\nfrom analytics.base.base_chargify_customer c\nleft join analytics.base.base_salesforce_limble_account la \n    on la.limble_customer_id = c.limble_customer_id \nleft join analytics.base.base_salesforce_account a\n    on a.salesforce_account_id = la.salesforce_account_id \ninner join analytics.base.base_saas_optics_customer cu -- do not return when customer not found in saas optics\n    on cu.saas_optics_customer_number = c.chargify_customer_id \nwhere (cu.is_active or la.customer_status in ('active','past_due','not_paid'))\n-- look for mismatches or where the saas optics field is null\nand ((cu.limble_customer_id != c.limble_customer_id or cu.limble_customer_id is null)\nor (cu.segment != a.segment or cu.segment is null)\nor (cu.industry != a.industry or cu.industry is null))\nand a.salesforce_account_id is not null -- do not update if there is no sfdc account found\nand ((cu.industry is null and a.industry is not null) or (cu.segment is null and a.segment is not null))  -- update when sfdc account is not null'''\n\napi_url = os.environ.get('api_url')\nrev_rec_write_token = os.environ.get('rev_rec_write_token')\n\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Token {rev_rec_write_token}\"\n}\n\n\n\n\n\ndef get_snowflake_data(query):\n    \"\"\"\n    Fetch data from Snowflake based on a given query.\n    \n    Parameters:\n        query (str): The SQL query to execute.\n        \n    Returns:\n        list: List of dictionaries containing query results.\n    \"\"\"\n    # Establish connection to Snowflake\n    conn = snowflake.connector.connect(\n        user = os.environ.get('snowflake_user'),\n        password = os.environ.get('snowflake_password'),\n        account = 'bingbong',\n        warehouse = 'transforming',\n        database = 'l_db',\n        schema = 'base'\n    )\n    \n    # Execute query\n    cursor = conn.cursor()\n    # Query for pulling the data from sfdc snowflake\n    cursor.execute(query)\n    \n    # Fetch results\n    results = cursor.fetchall()\n    columns = [col[0] for col in cursor.description]\n    \n    # Close connection\n    cursor.close()\n    conn.close()\n    \n    # Convert results to list of dictionaries\n    data = [dict(zip(columns, row)) for row in results]\n    return data\n\n\n\n\n\ndef partial_update_customer(api_url, saas_optics_customer_id, headers, name, **kwargs):\n    \"\"\"\n    Updates a specific customer resource using a PATCH request.\n    Parameters:\n        api_url (str): The base URL of the API.\n        resource_id (str): The ID of the customer resource to update.\n        headers (dict): The headers to include in the request (e.g., for authentication).\n        name (str): The name of the customer (required).\n        kwargs: Other fields to update (key-value pairs).\n    Returns:\n        dict: The response from the API if successful.\n        None: If the update fails.\n    \"\"\"\n    # Ensure the 'name' field is included in the data\n    data = {'name': name}\n\n    # Add any other fields to update\n    data.update(kwargs)\n\n    # Make the PATCH request\n    response = requests.patch(f\"{api_url}/{saas_optics_customer_id}/\", headers=headers, json=data)\n    # Log the response for debugging\n    print(f\"Status Code for ID {saas_optics_customer_id}: {response.status_code}\")\n    print(f\"Response Content for ID {saas_optics_customer_id}: {response.content.decode('utf-8')}\")\n    # Check the response\n    if response.status_code == 200:\n        return response.json()\n    else:\n        print(\"Failed to update:\", response.status_code, response.text)\n        return None\n\n\n\n\n\ndef update_customers_from_snowflake(api_url, headers, query):\n    \"\"\"\n    Fetch customer data from Snowflake and update each customer using the API.\n    Parameters:\n        api_url (str): The base URL of the API.\n        headers (dict): The headers to include in the request (e.g., for authentication).\n        query (str): The SQL query to fetch customer data from Snowflake.\n    \"\"\"\n    # Fetch data from Snowflake\n    customer_data = get_snowflake_data(query)\n    print(customer_data)\n    # Iterate through each customer record and update via API\n    for customer in customer_data:\n        print(customer)\n        saas_optics_customer_id = customer['SAAS_OPTICS_CUSTOMER_ID']\n        name = customer['CUSTOMER_NAME']\n        additional_fields = {key.lower(): value for key, value in customer.items() if key not in ['SAAS_OPTICS_CUSTOMER_ID', 'CUSTOMER_NAME']}\n        print(additional_fields)\n        response = partial_update_customer(api_url, saas_optics_customer_id, headers, name, **additional_fields)\n        if response:\n            print(\"Update successful for ID:\", saas_optics_customer_id)\n        else:\n            print(\"Update failed for ID:\", saas_optics_customer_id)\n\n# Run the method and see the results printed out\n# update_customers_from_snowflake(api_url, headers, query)"
  },
  {
    "objectID": "python_scripts.html#test-title",
    "href": "python_scripts.html#test-title",
    "title": "Python Data Scripts",
    "section": "Test title",
    "text": "Test title\n\ndef get_snowflake_data(query):\n    \"\"\"\n    Fetch data from Snowflake based on a given query.\n    \n    Parameters:\n        query (str): The SQL query to execute.\n        \n    Returns:\n        list: List of dictionaries containing query results.\n    \"\"\"\n    # Establish connection to Snowflake\n    conn = snowflake.connector.connect(\n        user = os.environ.get('snowflake_user'),\n        password = os.environ.get('snowflake_password'),\n        account = 'bingbong',\n        warehouse = 'transforming',\n        database = 'l_db',\n        schema = 'base'\n    )\n    \n    # Execute query\n    cursor = conn.cursor()\n    # Query for pulling the data from sfdc snowflake\n    cursor.execute(query)\n    \n    # Fetch results\n    results = cursor.fetchall()\n    columns = [col[0] for col in cursor.description]\n    \n    # Close connection\n    cursor.close()\n    conn.close()\n    \n    # Convert results to list of dictionaries\n    data = [dict(zip(columns, row)) for row in results]\n    return data"
  },
  {
    "objectID": "python_scripts.html#tester",
    "href": "python_scripts.html#tester",
    "title": "Python Data Scripts",
    "section": "tester",
    "text": "tester\n:::{.callout-caution collapse = true}\n\ndef get_snowflake_data(query):\n    \"\"\"\n    Fetch data from Snowflake based on a given query.\n    \n    Parameters:\n        query (str): The SQL query to execute.\n        \n    Returns:\n        list: List of dictionaries containing query results.\n    \"\"\"\n    # Establish connection to Snowflake\n    conn = snowflake.connector.connect(\n        user = os.environ.get('snowflake_user'),\n        password = os.environ.get('snowflake_password'),\n        account = 'bingbong',\n        warehouse = 'transforming',\n        database = 'l_db',\n        schema = 'base'\n    )\n    \n    # Execute query\n    cursor = conn.cursor()\n    # Query for pulling the data from sfdc snowflake\n    cursor.execute(query)\n    \n    # Fetch results\n    results = cursor.fetchall()\n    columns = [col[0] for col in cursor.description]\n    \n    # Close connection\n    cursor.close()\n    conn.close()\n    \n    # Convert results to list of dictionaries\n    data = [dict(zip(columns, row)) for row in results]\n    return data\n\n:::"
  },
  {
    "objectID": "python_scripts.html#yaml-file-used-in-github-actions",
    "href": "python_scripts.html#yaml-file-used-in-github-actions",
    "title": "Python Data Scripts",
    "section": "YAML file used in GitHub Actions",
    "text": "YAML file used in GitHub Actions\n\n'name: saas_optics_daily_update\n\non:\n  workflow_dispatch:\n  schedule:\n    - cron: '0 11 * * *'\n\njobs:\n  build:\n    environment: snowflake_python\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: checkout\n        uses: actions/checkout@v4\n\n      - uses: actions/setup-python@v3\n        with:\n          python-version: '3.8'\n\n      - name: install requirements\n        run: pip install -q -r requirements.txt\n\n      - name: Run Python script\n        env:\n            snowflake_user: ${{ secrets.SNOWFLAKE_USER }}\n            snowflake_password: ${{ secrets.SNOWFLAKE_PASSWORD }}\n            api_url: ${{ secrets.SAAS_OPTICS_WRITE_API }}\n            saas_optics_write_token: ${{ secrets.SAAS_OPTICS_WRITE_TOKEN }}\n        run: |\n            python3 -u python/saas_optics_customer_update.py\n\n      - name: Post to a Slack channel_success\n        if: ${{ success() }}\n        id: slack_success\n        uses: slackapi/slack-github-action@v1.24.0\n        with:\n          channel-id: 'C062M0PPZDZ'\n          slack-message: \":white_check_mark:  Job saas_optics_daily_update result: ${{ job.status }}\\n${{ github.event.pull_request.html_url || github.event.head_commit.url }}\"\n        env:\n          SLACK_BOT_TOKEN: ${{ secrets.SLACK_BOT_TOKEN }}\n\n      - name: Post to a Slack channel_failure\n        if: ${{ failure() }}\n        id: slack_failure\n        uses: slackapi/slack-github-action@v1.24.0\n        with:\n          channel-id: 'C062M0PPZDZ'\n          slack-message: \":red-light-blinker: &lt;!subteam^S06FP8WBJRH|@dwh-team&gt; Job saas_optics_daily_update result: ${{ job.status }}\\n${{ github.event.pull_request.html_url || github.event.head_commit.url }}\"\n        env:\n          SLACK_BOT_TOKEN: ${{ secrets.SLACK_BOT_TOKEN }}\n          '"
  }
]